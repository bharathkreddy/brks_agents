{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "786c3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from google import genai\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624a621",
   "metadata": {},
   "source": [
    "### Using local models - ollama\n",
    "Run `ollama run deepseek-r1:8b` locally on terminal before doing \n",
    "\n",
    "Visiting `http://localhost:11434` should give `Ollama is running`, if not then `ollama serve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fafcef53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Greetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"deepseek-r1:8b\"\n",
    "ollama_model = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "response = ollama_model.chat.completions.create(\n",
    " model=MODEL,\n",
    " messages=[{\"role\": \"user\", \"content\": \"who are you ?\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4ada5a",
   "metadata": {},
   "source": [
    "### OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bdf2ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! Welcome, and thanks for saying hi. I’m ChatGPT—here to help answer questions, brainstorm ideas, or just chat. What can I do for you today?\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "message = \"Hello, GPT! This is my first ever message to you! Hi!\"\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o4-mini\",\n",
    "    messages=[{\"role\":\"user\", \"content\":message}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133c6da",
   "metadata": {},
   "source": [
    "### AI web summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "431a333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some websites need a proper header when fetching them, below header is copied from dev tools under request headers\n",
    "# BeautifulSoup is ideal for static pages and simple scraping tasks, offering speed and efficiency. Selenium is better for dynamic content or tasks requiring user interaction, like form submission or clicking buttons.\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using BeautifulSoupe Library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelavent in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelavent.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        self.soup = soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77580e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bharath Reddy - Northern Trust | LinkedIn\n",
      "LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including\n",
      "professional and job ads\n",
      ") on and off LinkedIn. Learn more in our\n",
      "Cookie Policy\n",
      ".\n",
      "Select Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your\n",
      "settings\n",
      ".\n",
      "Accept\n",
      "Reject\n",
      "Skip to main content\n",
      "LinkedIn\n",
      "Top Content\n",
      "People\n",
      "Learning\n",
      "Jobs\n",
      "Games\n",
      "Get the app\n",
      "Join now\n",
      "Sign in\n",
      "Sign in to view Bharath’s full profile\n",
      "Sign in\n",
      "Welcome back\n",
      "Email or phone\n",
      "Password\n",
      "Show\n",
      "Forgot password?\n",
      "Sign in\n",
      "or\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "or\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "Bharath Reddy\n",
      "Sign in to view Bharath’s full profile\n",
      "Sign in\n",
      "Welcome back\n",
      "Email or phone\n",
      "Password\n",
      "Show\n",
      "Forgot password?\n",
      "Sign in\n",
      "or\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "or\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "London Area, United Kingdom\n",
      "Contact Info\n",
      "Sign in to view Bharath’s full profile\n",
      "Sign in\n",
      "Welcome back\n",
      "Email or phone\n",
      "Password\n",
      "Show\n",
      "Forgot password?\n",
      "Sign in\n",
      "or\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "or\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "18K followers\n",
      "500+ connections\n",
      "See your mutual connections\n",
      "View mutual connections with Bharath\n",
      "Sign in\n",
      "Welcome back\n",
      "Email or phone\n",
      "Password\n",
      "Show\n",
      "Forgot password?\n",
      "Sign in\n",
      "or\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "or\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "Join to view profile\n",
      "Message\n",
      "Sign in to view Bharath’s full profile\n",
      "Sign in\n",
      "Welcome back\n",
      "Email or phone\n",
      "Password\n",
      "Show\n",
      "Forgot password?\n",
      "Sign in\n",
      "or\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "or\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "Northern Trust\n",
      "Caltech\n",
      "Personal Website\n",
      "Report this profile\n",
      "About\n",
      "I use data and quantitative methods to solve business problems. My expertise is at the…\n",
      "see more\n",
      "Welcome back\n",
      "Email or phone\n",
      "Password\n",
      "Show\n",
      "Forgot password?\n",
      "Sign in\n",
      "or\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "Activity\n",
      "Follow\n",
      "Sign in to view Bharath’s full profile\n",
      "Sign in\n",
      "Welcome back\n",
      "Email or phone\n",
      "Password\n",
      "Show\n",
      "Forgot password?\n",
      "Sign in\n",
      "or\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "or\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "### 🧠 **10 Cutting-Edge Knowledge Graph Architectures (2025)**  \n",
      "\n",
      "1️⃣ **Domain Knowledge Graphs**  \n",
      "- **Architecture:** Industry-specific ontologies…\n",
      "### 🧠 **10 Cutting-Edge Knowledge Graph Architectures (2025)**  \n",
      "\n",
      "1️⃣ **Domain Knowledge Graphs**  \n",
      "- **Architecture:** Industry-specific ontologies…\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "Fine-tuning massive LLMs used to be painfully slow, but not anymore!\n",
      "\n",
      "4 open source libraries that accelerate fine-tuning of Large Language…\n",
      "Fine-tuning massive LLMs used to be painfully slow, but not anymore!\n",
      "\n",
      "4 open source libraries that accelerate fine-tuning of Large Language…\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "🚨🚨🚨Breaking!!! Today, Anthropic officially launched Claude for Financial Services!!\n",
      "\n",
      "This is a powerful, enterprise‑grade AI tailored for banks…\n",
      "🚨🚨🚨Breaking!!! Today, Anthropic officially launched Claude for Financial Services!!\n",
      "\n",
      "This is a powerful, enterprise‑grade AI tailored for banks…\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "Join now to see all activity\n",
      "Experience & Education\n",
      "Northern Trust\n",
      "****** **** *********\n",
      "***.*************.***\n",
      "*******\n",
      "******** ***** ***********\n",
      "**** ******* **** *********\n",
      "*******\n",
      "**** ******** ******* ** ****** ******\n",
      "2021\n",
      "-\n",
      "2022\n",
      "*** ********** ** ***** ** ******\n",
      "************ ****** **** ******** ******* ** ********** ************ *** ******* ******** *\n",
      "2020\n",
      "-\n",
      "2021\n",
      "View Bharath’s full experience\n",
      "See their title, tenure and more.\n",
      "Sign in\n",
      "Welcome back\n",
      "Email or phone\n",
      "Password\n",
      "Show\n",
      "Forgot password?\n",
      "Sign in\n",
      "or\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "or\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "Licenses & Certifications\n",
      "Getting and Cleaning Data\n",
      "Johns Hopkins University Data Science Laboratory\n",
      "Issued\n",
      "Apr 2019\n",
      "Credential ID FSSCJXHKRZNH\n",
      "See credential\n",
      "Practical Machine Learning\n",
      "Johns Hopkins University Data Science Laboratory\n",
      "Issued\n",
      "Apr 2019\n",
      "Credential ID 8DLEC4UXVSU3\n",
      "See credential\n",
      "R Programming\n",
      "Coursera Course Certificates\n",
      "Issued\n",
      "Apr 2019\n",
      "Credential ID YWC3YN49ZJPW\n",
      "See credential\n",
      "Regression Models\n",
      "Coursera Course Certificates\n",
      "Issued\n",
      "Apr 2019\n",
      "Credential ID 42JEBPMPPYAP\n",
      "See credential\n",
      "The R Programming Environment\n",
      "Coursera Course Certificates\n",
      "Issued\n",
      "Apr 2019\n",
      "Credential ID G8DPE9UFGVG3\n",
      "See credential\n",
      "Reproducible Research\n",
      "Johns Hopkins University Data Science Laboratory\n",
      "Issued\n",
      "Mar 2019\n",
      "Credential ID LXYT3DLLWYA5\n",
      "See credential\n",
      "Fundamentals of Visualization with Tableau\n",
      "Coursera Course Certificates\n",
      "Issued\n",
      "Mar 2019\n",
      "Credential ID B3NEY6AZDLHP\n",
      "See credential\n",
      "The Data Scientist’s Toolbox\n",
      "Johns Hopkins University Advanced Academic Programs\n",
      "Issued\n",
      "Mar 2019\n",
      "Credential ID SV3EAFUML88E\n",
      "See credential\n",
      "Statistical Inference\n",
      "Coursera Course Certificates\n",
      "Issued\n",
      "Feb 2019\n",
      "Credential ID LT9ANA6B3PMW\n",
      "See credential\n",
      "Business Metrics for Data-Driven Companies\n",
      "Duke University\n",
      "Issued\n",
      "Mar 2018\n",
      "Credential ID 6H8T6MTYX7KW\n",
      "See credential\n",
      "Join now to see all certifications\n",
      "Recommendations received\n",
      "Sushant Chatterjee\n",
      "“Bharat has always been a fantastic resource, a keen intellectual, and a terrific individual who thinks out of box. During college, I noticed the way how he will add resources and find links for whichever projects given to him with elan and ease! He combines rich literary words along with strong analytical / mathematical researches - a rare combination :) He always flourish under challenges, and apart from excelling in academics - than man also possesses a nice human heart and friendly banter. A strong people's manager, I wish him lot of success in his career and life.”\n",
      "1 person has recommended Bharath\n",
      "Join now to view\n",
      "More activity by Bharath\n",
      "If you're a software engineer who want to get better at algorithms and system design, read these 12 curated articles: \n",
      "\n",
      "1. What every engineer should…\n",
      "If you're a software engineer who want to get better at algorithms and system design, read these 12 curated articles: \n",
      "\n",
      "1. What every engineer should…\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "If you want to improve your coding skills, check these 5 GitHub repos: \n",
      "\n",
      "1. Awesome Software and Architectural Design Patterns\n",
      "\n",
      "A curated list of…\n",
      "If you want to improve your coding skills, check these 5 GitHub repos: \n",
      "\n",
      "1. Awesome Software and Architectural Design Patterns\n",
      "\n",
      "A curated list of…\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "What if I told you that you could slash your multi-vector embedding costs by 70%?\n",
      "(and still maintain solid performance?)\n",
      "\n",
      "Multi-vector models like…\n",
      "What if I told you that you could slash your multi-vector embedding costs by 70%?\n",
      "(and still maintain solid performance?)\n",
      "\n",
      "Multi-vector models like…\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "If you want to become a top-tier software developer, read these 10 books:\n",
      "\n",
      "1. The Pragmatic Programmer by Andrew Hunt and David Thomas \n",
      "\n",
      "It'll teach…\n",
      "If you want to become a top-tier software developer, read these 10 books:\n",
      "\n",
      "1. The Pragmatic Programmer by Andrew Hunt and David Thomas \n",
      "\n",
      "It'll teach…\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "These Leetcode blogs will tell you every pattern & tactic you need to know to solve Leetcode problems. If you don't want to solve 500+ Leetcode…\n",
      "These Leetcode blogs will tell you every pattern & tactic you need to know to solve Leetcode problems. If you don't want to solve 500+ Leetcode…\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "This is hands down the best playlist I found for Deep Learning \n",
      "\n",
      "This is taught by Prof. Bryce, the GOAT in his field.\n",
      "\n",
      "I've put together the list of…\n",
      "This is hands down the best playlist I found for Deep Learning \n",
      "\n",
      "This is taught by Prof. Bryce, the GOAT in his field.\n",
      "\n",
      "I've put together the list of…\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "If I had to go from zero to building reliable agentic systems in 90 days, this is the exact learning arc I’d follow.\n",
      "\n",
      "You don’t “learn AI.” You build…\n",
      "If I had to go from zero to building reliable agentic systems in 90 days, this is the exact learning arc I’d follow.\n",
      "\n",
      "You don’t “learn AI.” You build…\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "Last year I wrote seven tutorials on infinite-width networks for RBC Borealis:\n",
      "\n",
      "https://lnkd.in/eEmBJRhV\n",
      "\n",
      "Parts I-III discuss the Neural Tangent…\n",
      "Last year I wrote seven tutorials on infinite-width networks for RBC Borealis:\n",
      "\n",
      "https://lnkd.in/eEmBJRhV\n",
      "\n",
      "Parts I-III discuss the Neural Tangent…\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "12 Blogs That Changed How I Think About LLMs & RAG  \n",
      "\n",
      "Six months into working with LLMs, I hit the ceiling. Tutorials felt shallow. Docs were too…\n",
      "12 Blogs That Changed How I Think About LLMs & RAG  \n",
      "\n",
      "Six months into working with LLMs, I hit the ceiling. Tutorials felt shallow. Docs were too…\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "𝗧𝗶𝗺𝗲 𝗦𝗲𝗿𝗶𝗲𝘀 𝗙𝗼𝗿𝗲𝗰𝗮𝘀𝘁𝗶𝗻𝗴 𝘄𝗶𝘁𝗵 𝗗𝗲𝗲𝗽 𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴!💡📈\n",
      "In recent years, deep learning has been widely used for time…\n",
      "𝗧𝗶𝗺𝗲 𝗦𝗲𝗿𝗶𝗲𝘀 𝗙𝗼𝗿𝗲𝗰𝗮𝘀𝘁𝗶𝗻𝗴 𝘄𝗶𝘁𝗵 𝗗𝗲𝗲𝗽 𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴!💡📈\n",
      "In recent years, deep learning has been widely used for time…\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "This is a great strategy 🤩\n",
      "This is a great strategy 🤩\n",
      "Liked by\n",
      "Bharath Reddy\n",
      "View Bharath’s full profile\n",
      "See who you know in common\n",
      "Get introduced\n",
      "Contact Bharath directly\n",
      "Join to view full profile\n",
      "Sign in\n",
      "Stay updated on your professional world\n",
      "Sign in\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "Other similar profiles\n",
      "Josly Dhanraj Fernandes\n",
      "Second Vice President, Senior Consultant at Northern Trust Corporation\n",
      "Bengaluru\n",
      "Connect\n",
      "Madhan MV\n",
      "Actively Looking for Employment | Experienced Writer with more than 9 Years of experience | Content Writing | Technical Writing | Creative Writing | Blogs | Articles | Web Content | Research Papers\n",
      "Bengaluru\n",
      "Connect\n",
      "Rabia Ahmad\n",
      "IIM Nagpur | Prince2 Agile| Data Quality| Compliance| Payments\n",
      "Bengaluru\n",
      "Connect\n",
      "Kshitij Singh\n",
      "Mumbai\n",
      "Connect\n",
      "Javeed Nadaf\n",
      "Vice President II | Lead Manager Middle Office Corporate Action and Client Service Delivery at BNY International Operations (India) Private Ltd.\n",
      "Pune\n",
      "Connect\n",
      "Nithin M\n",
      "Officer, Team Leader | Global Derivatives Practice | Collateral Management | Leadership\n",
      "Bengaluru\n",
      "Connect\n",
      "Naveen Sai P\n",
      "Mumbai\n",
      "Connect\n",
      "Atul Sashittal\n",
      "Mumbai Metropolitan Region\n",
      "Connect\n",
      "Ranjeetha Rao\n",
      "Assistant Vice President in Hedge Fund Accounting  at Statestreet Corporation\n",
      "Bengaluru\n",
      "Connect\n",
      "Explore top content on LinkedIn\n",
      "Find curated posts and insights for relevant topics all in one place.\n",
      "View top content\n",
      "Others named\n",
      "Bharath Reddy\n",
      "in\n",
      "United Kingdom\n",
      "Bharath Reddy\n",
      "London\n",
      "Bharath Reddy\n",
      "Data Engineer | Pyspark | Databricks | Cloud, BI, ETL Expert\n",
      "United Kingdom\n",
      "Bharath reddy\n",
      "London Area, United Kingdom\n",
      "Bharath Reddy\n",
      "Civil Engineer | MSc Construction Project & Cost Management | Temporary Works | Tunnels | HS2\n",
      "United Kingdom\n",
      "29 others named Bharath Reddy in United Kingdom are on LinkedIn\n",
      "See others named\n",
      "Bharath Reddy\n",
      "Add new skills with these courses\n",
      "2h 45m\n",
      "Learning Data Science\n",
      "3h 8m\n",
      "Build a No-Code ETL Pipeline with Google BigQuery\n",
      "2h 29m\n",
      "Big Data in the Age of AI\n",
      "See all courses\n",
      "LinkedIn\n",
      "© 2025\n",
      "About\n",
      "Accessibility\n",
      "User Agreement\n",
      "Privacy Policy\n",
      "Cookie Policy\n",
      "Copyright Policy\n",
      "Brand Policy\n",
      "Guest Controls\n",
      "Community Guidelines\n",
      "العربية (Arabic)\n",
      "বাংলা (Bangla)\n",
      "Čeština (Czech)\n",
      "Dansk (Danish)\n",
      "Deutsch (German)\n",
      "Ελληνικά (Greek)\n",
      "English (English)\n",
      "Español (Spanish)\n",
      "فارسی (Persian)\n",
      "Suomi (Finnish)\n",
      "Français (French)\n",
      "हिंदी (Hindi)\n",
      "Magyar (Hungarian)\n",
      "Bahasa Indonesia (Indonesian)\n",
      "Italiano (Italian)\n",
      "עברית (Hebrew)\n",
      "日本語 (Japanese)\n",
      "한국어 (Korean)\n",
      "मराठी (Marathi)\n",
      "Bahasa Malaysia (Malay)\n",
      "Nederlands (Dutch)\n",
      "Norsk (Norwegian)\n",
      "ਪੰਜਾਬੀ (Punjabi)\n",
      "Polski (Polish)\n",
      "Português (Portuguese)\n",
      "Română (Romanian)\n",
      "Русский (Russian)\n",
      "Svenska (Swedish)\n",
      "తెలుగు (Telugu)\n",
      "ภาษาไทย (Thai)\n",
      "Tagalog (Tagalog)\n",
      "Türkçe (Turkish)\n",
      "Українська (Ukrainian)\n",
      "Tiếng Việt (Vietnamese)\n",
      "简体中文 (Chinese (Simplified))\n",
      "正體中文 (Chinese (Traditional))\n",
      "Language\n",
      "Agree & Join LinkedIn\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "View Bharath’s full profile\n",
      "Sign in\n",
      "Welcome back\n",
      "Email or phone\n",
      "Password\n",
      "Show\n",
      "Forgot password?\n",
      "Sign in\n",
      "or\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "or\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "LinkedIn\n",
      "LinkedIn is better on the app\n",
      "Don’t have the app? Get it in the Microsoft Store.\n",
      "Open the app\n"
     ]
    }
   ],
   "source": [
    "brk = Website(\"https://www.linkedin.com/in/bharath-k-reddy/\")\n",
    "print(brk.title)\n",
    "print(brk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4602b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyses the contents of a website \\\n",
    "    and provides a short summary, ignoring text that might be navigation related. \\\n",
    "    Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "361b2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled: {website.title}\"\n",
    "    user_prompt += \"The contents of this website is as follows; \\\n",
    "        please provide a short summary of this website in markdown.\\\n",
    "        if it includes news or announcements, then summarize these too. \\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acaf5c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are looking at a website titled: How we built our multi-agent research system \\\\ AnthropicThe contents of this website is as follows;         please provide a short summary of this website in markdown.        if it includes news or announcements, then summarize these too. \\n\\nSkip to main content\\nSkip to footer\\nClaude\\nAPI\\nSolutions\\nResearch\\nCommitments\\nLearn\\nNews\\nTry Claude\\nEngineering at Anthropic\\nHow we built our multi-agent research system\\nPublished\\nJun 13, 2025\\nOur Research feature uses multiple Claude agents to explore complex topics more effectively. We share the engineering challenges and the lessons we learned from building this system.\\nClaude now has\\nResearch capabilities\\nthat allow it to search across the web, Google Workspace, and any integrations to accomplish complex tasks.\\nThe journey of this multi-agent system from prototype to production taught us critical lessons about system architecture, tool design, and prompt engineering. A multi-agent system consists of multiple agents (LLMs autonomously using tools in a loop) working together. Our Research feature involves an agent that plans a research process based on user queries, and then uses tools to create parallel agents that search for information simultaneously. Systems with multiple agents introduce new challenges in agent coordination, evaluation, and reliability.\\nThis post breaks down the principles that worked for us—we hope you'll find them useful to apply when building your own multi-agent systems.\\nBenefits of a multi-agent system\\nResearch work involves open-ended problems where it’s very difficult to predict the required steps in advance. You can’t hardcode a fixed path for exploring complex topics, as the process is inherently dynamic and path-dependent. When people conduct research, they tend to continuously update their approach based on discoveries, following leads that emerge during investigation.\\nThis unpredictability makes AI agents particularly well-suited for research tasks. Research demands the flexibility to pivot or explore tangential connections as the investigation unfolds. The model must operate autonomously for many turns, making decisions about which directions to pursue based on intermediate findings. A linear, one-shot pipeline cannot handle these tasks.\\nThe essence of search is compression: distilling insights from a vast corpus. Subagents facilitate compression by operating in parallel with their own context windows, exploring different aspects of the question simultaneously before condensing the most important tokens for the lead research agent. Each subagent also provides separation of concerns—distinct tools, prompts, and exploration trajectories—which reduces path dependency and enables thorough, independent investigations.\\nOnce intelligence reaches a threshold, multi-agent systems become a vital way to scale performance. For instance, although individual humans have become more intelligent in the last 100,000 years, human societies have become\\nexponentially\\nmore capable in the information age because of our\\ncollective\\nintelligence and ability to coordinate. Even generally-intelligent agents face limits when operating as individuals; groups of agents can accomplish far more.\\nOur internal evaluations show that multi-agent research systems excel especially for breadth-first queries that involve pursuing multiple independent directions simultaneously. We found that a multi-agent system with Claude Opus 4 as the lead agent and Claude Sonnet 4 subagents outperformed single-agent Claude Opus 4 by 90.2% on our internal research eval. For example, when asked to identify all the board members of the companies in the Information Technology S&P 500, the multi-agent system found the correct answers by decomposing this into tasks for subagents, while the single agent system failed to find the answer with slow, sequential searches.\\nMulti-agent systems work mainly because they help spend enough tokens to solve the problem. In our analysis, three factors explained 95% of the performance variance in the\\nBrowseComp\\nevaluation (which tests the ability of browsing agents to locate hard-to-find information). We found that token usage by itself explains 80% of the variance, with the number of tool calls and the model choice as the two other explanatory factors. This finding validates our architecture that distributes work across agents with separate context windows to add more capacity for parallel reasoning. The latest Claude models act as large efficiency multipliers on token use, as upgrading to Claude Sonnet 4 is a larger performance gain than doubling the token budget on Claude Sonnet 3.7. Multi-agent architectures effectively scale token usage for tasks that exceed the limits of single agents.\\nThere is a downside: in practice, these architectures burn through tokens fast. In our data, agents typically use about 4× more tokens than chat interactions, and multi-agent systems use about 15× more tokens than chats. For economic viability, multi-agent systems require tasks where the value of the task is high enough to pay for the increased performance. Further, some domains that require all agents to share the same context or involve many dependencies between agents are not a good fit for multi-agent systems today. For instance, most coding tasks involve fewer truly parallelizable tasks than research, and LLM agents are not yet great at coordinating and delegating to other agents in real time. We’ve found that multi-agent systems excel at valuable tasks that involve heavy parallelization, information that exceeds single context windows, and interfacing with numerous complex tools.\\nArchitecture overview for Research\\nOur Research system uses a multi-agent architecture with an orchestrator-worker pattern, where a lead agent coordinates the process while delegating to specialized subagents that operate in parallel.\\nThe multi-agent architecture in action: user queries flow through a lead agent that creates specialized subagents to search for different aspects in parallel.\\nWhen a user submits a query, the lead agent analyzes it, develops a strategy, and spawns subagents to explore different aspects simultaneously. As shown in the diagram above, the subagents act as intelligent filters by iteratively using search tools to gather information, in this case on AI agent companies in 2025, and then returning a list of companies to the lead agent so it can compile a final answer.\\nTraditional approaches using Retrieval Augmented Generation (RAG) use static retrieval. That is, they fetch some set of chunks that are most similar to an input query and use these chunks to generate a response. In contrast, our architecture uses a multi-step search that dynamically finds relevant information, adapts to new findings, and analyzes results to formulate high-quality answers.\\nProcess diagram showing the complete workflow of our multi-agent Research system. When a user submits a query, the system creates a LeadResearcher agent that enters an iterative research process. The LeadResearcher begins by thinking through the approach and saving its plan to Memory to persist the context, since if the context window exceeds 200,000 tokens it will be truncated and it is important to retain the plan. It then creates specialized Subagents (two are shown here, but it can be any number) with specific research tasks. Each Subagent independently performs web searches, evaluates tool results using\\ninterleaved thinking\\n, and returns findings to the LeadResearcher. The LeadResearcher synthesizes these results and decides whether more research is needed—if so, it can create additional subagents or refine its strategy. Once sufficient information is gathered, the system exits the research loop and passes all findings to a CitationAgent, which processes the documents and research report to identify specific locations for citations. This ensures all claims are properly attributed to their sources. The final research results, complete with citations, are then returned to the user.\\nPrompt engineering and evaluations for research agents\\nMulti-agent systems have key differences from single-agent systems, including a rapid growth in coordination complexity. Early agents made errors like spawning 50 subagents for simple queries, scouring the web endlessly for nonexistent sources, and distracting each other with excessive updates. Since each agent is steered by a prompt, prompt engineering was our primary lever for improving these behaviors. Below are some principles we learned for prompting agents:\\nThink like your agents.\\nTo iterate on prompts, you must understand their effects. To help us do this, we built simulations using our\\nConsole\\nwith the exact prompts and tools from our system, then watched agents work step-by-step. This immediately revealed failure modes: agents continuing when they already had sufficient results, using overly verbose search queries, or selecting incorrect tools. Effective prompting relies on developing an accurate mental model of the agent, which can make the most impactful changes obvious.\\nTeach the orchestrator how to delegate.\\nIn our system, the lead agent decomposes queries into subtasks and describes them to subagents. Each subagent needs an objective, an output format, guidance on the tools and sources to use, and clear task boundaries. Without detailed task descriptions, agents duplicate work, leave gaps, or fail to find necessary information. We started by allowing the lead agent to give simple, short instructions like 'research the semiconductor shortage,' but found these instructions often were vague enough that subagents misinterpreted the task or performed the exact same searches as other agents. For instance, one subagent explored the 2021 automotive chip crisis while 2 others duplicated work investigating current 2025 supply chains, without an effective division of labor.\\nScale effort to query complexity.\\nAgents struggle to judge appropriate effort for different tasks, so we embedded scaling rules in the prompts. Simple fact-finding requires just 1 agent with 3-10 tool calls, direct comparisons might need 2-4 subagents with 10-15 calls each, and complex research might use more than 10 subagents with clearly divided responsibilities. These explicit guidelines help the lead agent allocate resources efficiently and prevent overinvestment in simple queries, which was a common failure mode in our early versions.\\nTool design and selection are critical.\\nAgent-tool interfaces are as critical as human-computer interfaces. Using the right tool is efficient—often, it’s strictly necessary. For instance, an agent searching the web for context that only exists in Slack is doomed from the start. With\\nMCP servers\\nthat give the model access to external tools, this problem compounds, as agents encounter unseen tools with descriptions of wildly varying quality. We gave our agents explicit heuristics: for example, examine all available tools first, match tool usage to user intent, search the web for broad external exploration, or prefer specialized tools over generic ones. Bad tool descriptions can send agents down completely wrong paths, so each tool needs a distinct purpose and a clear description.\\nLet agents improve themselves\\n. We found that the Claude 4 models can be excellent prompt engineers. When given a prompt and a failure mode, they are able to diagnose why the agent is failing and suggest improvements. We even created a tool-testing agent—when given a flawed MCP tool, it attempts to use the tool and then rewrites the tool description to avoid failures. By testing the tool dozens of times, this agent found key nuances and bugs. This process for improving tool ergonomics resulted in a 40% decrease in task completion time for future agents using the new description, because they were able to avoid most mistakes.\\nStart wide, then narrow down.\\nSearch strategy should mirror expert human research: explore the landscape before drilling into specifics. Agents often default to overly long, specific queries that return few results. We counteracted this tendency by prompting agents to start with short, broad queries, evaluate what’s available, then progressively narrow focus.\\nGuide the thinking process.\\nExtended thinking mode\\n, which leads Claude to output additional tokens in a visible thinking process, can serve as a controllable scratchpad. The lead agent uses thinking to plan its approach, assessing which tools fit the task, determining query complexity and subagent count, and defining each subagent’s role. Our testing showed that extended thinking improved instruction-following, reasoning, and efficiency. Subagents also plan, then use\\ninterleaved thinking\\nafter tool results to evaluate quality, identify gaps, and refine their next query. This makes subagents more effective in adapting to any task.\\nParallel tool calling transforms speed and performance.\\nComplex research tasks naturally involve exploring many sources. Our early agents executed sequential searches, which was painfully slow. For speed, we introduced two kinds of parallelization: (1) the lead agent spins up 3-5 subagents in parallel rather than serially; (2) the subagents use 3+ tools in parallel. These changes cut research time by up to 90% for complex queries, allowing Research to do more work in minutes instead of hours while covering more information than other systems.\\nOur prompting strategy focuses on instilling good heuristics rather than rigid rules. We studied how skilled humans approach research tasks and encoded these strategies in our prompts—strategies like decomposing difficult questions into smaller tasks, carefully evaluating the quality of sources, adjusting search approaches based on new information, and recognizing when to focus on depth (investigating one topic in detail) vs. breadth (exploring many topics in parallel). We also proactively mitigated unintended side effects by setting explicit guardrails to prevent the agents from spiraling out of control. Finally, we focused on a fast iteration loop with observability and test cases.\\nEffective evaluation of agents\\nGood evaluations are essential for building reliable AI applications, and agents are no different. However, evaluating multi-agent systems presents unique challenges. Traditional evaluations often assume that the AI follows the same steps each time: given input X, the system should follow path Y to produce output Z. But multi-agent systems don't work this way. Even with identical starting points, agents might take completely different valid paths to reach their goal. One agent might search three sources while another searches ten, or they might use different tools to find the same answer. Because we don’t always know what the right steps are, we usually can't just check if agents followed the “correct” steps we prescribed in advance. Instead, we need flexible evaluation methods that judge whether agents achieved the right outcomes while also following a reasonable process.\\nStart evaluating immediately with small samples\\n. In early agent development, changes tend to have dramatic impacts because there is abundant low-hanging fruit. A prompt tweak might boost success rates from 30% to 80%. With effect sizes this large, you can spot changes with just a few test cases. We started with a set of about 20 queries representing real usage patterns. Testing these queries often allowed us to clearly see the impact of changes. We often hear that AI developer teams delay creating evals because they believe that only large evals with hundreds of test cases are useful. However, it’s best to start with small-scale testing right away with a few examples, rather than delaying until you can build more thorough evals.\\nLLM-as-judge evaluation scales when done well.\\nResearch outputs are difficult to evaluate programmatically, since they are free-form text and rarely have a single correct answer. LLMs are a natural fit for grading outputs. We used an LLM judge that evaluated each output against criteria in a rubric: factual accuracy (do claims match sources?), citation accuracy (do the cited sources match the claims?), completeness (are all requested aspects covered?), source quality (did it use primary sources over lower-quality secondary sources?), and tool efficiency (did it use the right tools a reasonable number of times?). We experimented with multiple judges to evaluate each component, but found that a single LLM call with a single prompt outputting scores from 0.0-1.0 and a pass-fail grade was the most consistent and aligned with human judgements. This method was especially effective when the eval test cases\\ndid\\nhave a clear answer, and we could use the LLM judge to simply check if the answer was correct (i.e. did it accurately list the pharma companies with the top 3 largest R&D budgets?). Using an LLM as a judge allowed us to scalably evaluate hundreds of outputs.\\nHuman evaluation catches what automation misses.\\nPeople testing agents find edge cases that evals miss. These include hallucinated answers on unusual queries, system failures, or subtle source selection biases. In our case, human testers noticed that our early agents consistently chose SEO-optimized content farms over authoritative but less highly-ranked sources like academic PDFs or personal blogs. Adding source quality heuristics to our prompts helped resolve this issue. Even in a world of automated evaluations, manual testing remains essential.\\nMulti-agent systems have emergent behaviors, which arise without specific programming. For instance, small changes to the lead agent can unpredictably change how subagents behave. Success requires understanding interaction patterns, not just individual agent behavior. Therefore, the best prompts for these agents are not just strict instructions, but frameworks for collaboration that define the division of labor, problem-solving approaches, and effort budgets. Getting this right relies on careful prompting and tool design, solid heuristics, observability, and tight feedback loops.\\nSee the\\nopen-source prompts in our Cookbook\\nfor example prompts from our system.\\nProduction reliability and engineering challenges\\nIn traditional software, a bug might break a feature, degrade performance, or cause outages. In agentic systems, minor changes cascade into large behavioral changes, which makes it remarkably difficult to write code for complex agents that must maintain state in a long-running process.\\nAgents are stateful and errors compound.\\nAgents can run for long periods of time, maintaining state across many tool calls. This means we need to durably execute code and handle errors along the way. Without effective mitigations, minor system failures can be catastrophic for agents. When errors occur, we can't just restart from the beginning: restarts are expensive and frustrating for users. Instead, we built systems that can resume from where the agent was when the errors occurred. We also use the model’s intelligence to handle issues gracefully: for instance, letting the agent know when a tool is failing and letting it adapt works surprisingly well. We combine the adaptability of AI agents built on Claude with deterministic safeguards like retry logic and regular checkpoints.\\nDebugging benefits from new approaches.\\nAgents make dynamic decisions and are non-deterministic between runs, even with identical prompts. This makes debugging harder. For instance, users would report agents “not finding obvious information,” but we couldn't see why. Were the agents using bad search queries? Choosing poor sources? Hitting tool failures? Adding full production tracing let us diagnose why agents failed and fix issues systematically. Beyond standard observability, we monitor agent decision patterns and interaction structures—all without monitoring the contents of individual conversations, to maintain user privacy. This high-level observability helped us diagnose root causes, discover unexpected behaviors, and fix common failures.\\nDeployment needs careful coordination.\\nAgent systems are highly stateful webs of prompts, tools, and execution logic that run almost continuously. This means that whenever we deploy updates, agents might be anywhere in their process. We therefore need to prevent our well-meaning code changes from breaking existing agents. We can’t update every agent to the new version at the same time. Instead, we use\\nrainbow deployments\\nto avoid disrupting running agents, by gradually shifting traffic from old to new versions while keeping both running simultaneously.\\nSynchronous execution creates bottlenecks.\\nCurrently, our lead agents execute subagents synchronously, waiting for each set of subagents to complete before proceeding. This simplifies coordination, but creates bottlenecks in the information flow between agents. For instance, the lead agent can’t steer subagents, subagents can’t coordinate, and the entire system can be blocked while waiting for a single subagent to finish searching. Asynchronous execution would enable additional parallelism: agents working concurrently and creating new subagents when needed. But this asynchronicity adds challenges in result coordination, state consistency, and error propagation across the subagents. As models can handle longer and more complex research tasks, we expect the performance gains will justify the complexity.\\nConclusion\\nWhen building AI agents, the last mile often becomes most of the journey. Codebases that work on developer machines require significant engineering to become reliable production systems. The compound nature of errors in agentic systems means that minor issues for traditional software can derail agents entirely. One step failing can cause agents to explore entirely different trajectories, leading to unpredictable outcomes. For all the reasons described in this post, the gap between prototype and production is often wider than anticipated.\\nDespite these challenges, multi-agent systems have proven valuable for open-ended research tasks. Users have said that Claude helped them find business opportunities they hadn’t considered, navigate complex healthcare options, resolve thorny technical bugs, and save up to days of work by uncovering research connections they wouldn't have found alone. Multi-agent research systems can operate reliably at scale with careful engineering, comprehensive testing, detail-oriented prompt and tool design, robust operational practices, and tight collaboration between research, product, and engineering teams who have a strong understanding of current agent capabilities. We're already seeing these systems transform how people solve complex problems.\\nA\\nClio\\nembedding plot showing the most common ways people are using the Research feature today. The top use case categories are developing software systems across specialized domains (10%), develop and optimize professional and technical content (8%), develop business growth and revenue generation strategies (8%), assist with academic research and educational material development (7%), and research and verify information about people, places, or organizations (5%).\\nAcknowlegements\\nWritten by Jeremy Hadfield, Barry Zhang, Kenneth Lien, Florian Scholz, Jeremy Fox, and Daniel Ford. This work reflects the collective efforts of several teams across Anthropic who made the Research feature possible. Special thanks go to the Anthropic apps engineering team, whose dedication brought this complex multi-agent system to production. We're also grateful to our early users for their excellent feedback.\\nAppendix\\nBelow are some additional miscellaneous tips for multi-agent systems.\\nEnd-state evaluation of agents that mutate state over many turns.\\nEvaluating agents that modify persistent state across multi-turn conversations presents unique challenges. Unlike read-only research tasks, each action can change the environment for subsequent steps, creating dependencies that traditional evaluation methods struggle to handle. We found success focusing on end-state evaluation rather than turn-by-turn analysis. Instead of judging whether the agent followed a specific process, evaluate whether it achieved the correct final state. This approach acknowledges that agents may find alternative paths to the same goal while still ensuring they deliver the intended outcome. For complex workflows, break evaluation into discrete checkpoints where specific state changes should have occurred, rather than attempting to validate every intermediate step.\\nLong-horizon conversation management.\\nProduction agents often engage in conversations spanning hundreds of turns, requiring careful context management strategies. As conversations extend, standard context windows become insufficient, necessitating intelligent compression and memory mechanisms. We implemented patterns where agents summarize completed work phases and store essential information in external memory before proceeding to new tasks. When context limits approach, agents can spawn fresh subagents with clean contexts while maintaining continuity through careful handoffs. Further, they can retrieve stored context like the research plan from their memory rather than losing previous work when reaching the context limit. This distributed approach prevents context overflow while preserving conversation coherence across extended interactions.\\nSubagent output to a filesystem to minimize the ‘game of telephone.’\\nDirect subagent outputs can bypass the main coordinator for certain types of results, improving both fidelity and performance. Rather than requiring subagents to communicate everything through the lead agent, implement artifact systems where specialized agents can create outputs that persist independently. Subagents call tools to store their work in external systems, then pass lightweight references back to the coordinator. This prevents information loss during multi-stage processing and reduces token overhead from copying large outputs through conversation history. The pattern works particularly well for structured outputs like code, reports, or data visualizations where the subagent's specialized prompt produces better results than filtering through a general coordinator.\\nProduct\\nClaude overview\\nClaude Code\\nMax plan\\nTeam plan\\nEnterprise plan\\nDownload Claude apps\\nClaude.ai pricing plans\\nClaude.ai login\\nAPI Platform\\nAPI overview\\nDeveloper docs\\nClaude in Amazon Bedrock\\nClaude on Google Cloud's Vertex AI\\nPricing\\nConsole login\\nResearch\\nResearch overview\\nEconomic Index\\nClaude models\\nClaude Opus 4\\nClaude Sonnet 4\\nClaude Haiku 3.5\\nCommitments\\nTransparency\\nResponsible scaling policy\\nSecurity and compliance\\nSolutions\\nAI agents\\nCoding\\nCustomer support\\nEducation\\nFinancial services\\nLearn\\nAnthropic Academy\\nCustomer stories\\nEngineering at Anthropic\\nMCP Integrations\\nPartner Directory\\nExplore\\nAbout us\\nBecome a partner\\nCareers\\nEvents\\nNews\\nStartups program\\nHelp and security\\nStatus\\nAvailability\\nSupport center\\nTerms and policies\\nPrivacy choices\\nPrivacy policy\\nResponsible disclosure policy\\nTerms of service - consumer\\nTerms of service - commercial\\nUsage policy\\n© 2025 Anthropic PBC\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog = Website(\"https://www.anthropic.com/engineering/built-multi-agent-research-system?utm_source=alphasignal&utm_campaign=2025-07-18&asuniq=02f5e591\")\n",
    "user_prompt_for(blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd0d78b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an assistant that analyses the contents of a website     and provides a short summary, ignoring text that might be navigation related.     Respond in markdown.'},\n",
       " {'role': 'user',\n",
       "  'content': \"You are looking at a website titled: How we built our multi-agent research system \\\\ AnthropicThe contents of this website is as follows;         please provide a short summary of this website in markdown.        if it includes news or announcements, then summarize these too. \\n\\nSkip to main content\\nSkip to footer\\nClaude\\nAPI\\nSolutions\\nResearch\\nCommitments\\nLearn\\nNews\\nTry Claude\\nEngineering at Anthropic\\nHow we built our multi-agent research system\\nPublished\\nJun 13, 2025\\nOur Research feature uses multiple Claude agents to explore complex topics more effectively. We share the engineering challenges and the lessons we learned from building this system.\\nClaude now has\\nResearch capabilities\\nthat allow it to search across the web, Google Workspace, and any integrations to accomplish complex tasks.\\nThe journey of this multi-agent system from prototype to production taught us critical lessons about system architecture, tool design, and prompt engineering. A multi-agent system consists of multiple agents (LLMs autonomously using tools in a loop) working together. Our Research feature involves an agent that plans a research process based on user queries, and then uses tools to create parallel agents that search for information simultaneously. Systems with multiple agents introduce new challenges in agent coordination, evaluation, and reliability.\\nThis post breaks down the principles that worked for us—we hope you'll find them useful to apply when building your own multi-agent systems.\\nBenefits of a multi-agent system\\nResearch work involves open-ended problems where it’s very difficult to predict the required steps in advance. You can’t hardcode a fixed path for exploring complex topics, as the process is inherently dynamic and path-dependent. When people conduct research, they tend to continuously update their approach based on discoveries, following leads that emerge during investigation.\\nThis unpredictability makes AI agents particularly well-suited for research tasks. Research demands the flexibility to pivot or explore tangential connections as the investigation unfolds. The model must operate autonomously for many turns, making decisions about which directions to pursue based on intermediate findings. A linear, one-shot pipeline cannot handle these tasks.\\nThe essence of search is compression: distilling insights from a vast corpus. Subagents facilitate compression by operating in parallel with their own context windows, exploring different aspects of the question simultaneously before condensing the most important tokens for the lead research agent. Each subagent also provides separation of concerns—distinct tools, prompts, and exploration trajectories—which reduces path dependency and enables thorough, independent investigations.\\nOnce intelligence reaches a threshold, multi-agent systems become a vital way to scale performance. For instance, although individual humans have become more intelligent in the last 100,000 years, human societies have become\\nexponentially\\nmore capable in the information age because of our\\ncollective\\nintelligence and ability to coordinate. Even generally-intelligent agents face limits when operating as individuals; groups of agents can accomplish far more.\\nOur internal evaluations show that multi-agent research systems excel especially for breadth-first queries that involve pursuing multiple independent directions simultaneously. We found that a multi-agent system with Claude Opus 4 as the lead agent and Claude Sonnet 4 subagents outperformed single-agent Claude Opus 4 by 90.2% on our internal research eval. For example, when asked to identify all the board members of the companies in the Information Technology S&P 500, the multi-agent system found the correct answers by decomposing this into tasks for subagents, while the single agent system failed to find the answer with slow, sequential searches.\\nMulti-agent systems work mainly because they help spend enough tokens to solve the problem. In our analysis, three factors explained 95% of the performance variance in the\\nBrowseComp\\nevaluation (which tests the ability of browsing agents to locate hard-to-find information). We found that token usage by itself explains 80% of the variance, with the number of tool calls and the model choice as the two other explanatory factors. This finding validates our architecture that distributes work across agents with separate context windows to add more capacity for parallel reasoning. The latest Claude models act as large efficiency multipliers on token use, as upgrading to Claude Sonnet 4 is a larger performance gain than doubling the token budget on Claude Sonnet 3.7. Multi-agent architectures effectively scale token usage for tasks that exceed the limits of single agents.\\nThere is a downside: in practice, these architectures burn through tokens fast. In our data, agents typically use about 4× more tokens than chat interactions, and multi-agent systems use about 15× more tokens than chats. For economic viability, multi-agent systems require tasks where the value of the task is high enough to pay for the increased performance. Further, some domains that require all agents to share the same context or involve many dependencies between agents are not a good fit for multi-agent systems today. For instance, most coding tasks involve fewer truly parallelizable tasks than research, and LLM agents are not yet great at coordinating and delegating to other agents in real time. We’ve found that multi-agent systems excel at valuable tasks that involve heavy parallelization, information that exceeds single context windows, and interfacing with numerous complex tools.\\nArchitecture overview for Research\\nOur Research system uses a multi-agent architecture with an orchestrator-worker pattern, where a lead agent coordinates the process while delegating to specialized subagents that operate in parallel.\\nThe multi-agent architecture in action: user queries flow through a lead agent that creates specialized subagents to search for different aspects in parallel.\\nWhen a user submits a query, the lead agent analyzes it, develops a strategy, and spawns subagents to explore different aspects simultaneously. As shown in the diagram above, the subagents act as intelligent filters by iteratively using search tools to gather information, in this case on AI agent companies in 2025, and then returning a list of companies to the lead agent so it can compile a final answer.\\nTraditional approaches using Retrieval Augmented Generation (RAG) use static retrieval. That is, they fetch some set of chunks that are most similar to an input query and use these chunks to generate a response. In contrast, our architecture uses a multi-step search that dynamically finds relevant information, adapts to new findings, and analyzes results to formulate high-quality answers.\\nProcess diagram showing the complete workflow of our multi-agent Research system. When a user submits a query, the system creates a LeadResearcher agent that enters an iterative research process. The LeadResearcher begins by thinking through the approach and saving its plan to Memory to persist the context, since if the context window exceeds 200,000 tokens it will be truncated and it is important to retain the plan. It then creates specialized Subagents (two are shown here, but it can be any number) with specific research tasks. Each Subagent independently performs web searches, evaluates tool results using\\ninterleaved thinking\\n, and returns findings to the LeadResearcher. The LeadResearcher synthesizes these results and decides whether more research is needed—if so, it can create additional subagents or refine its strategy. Once sufficient information is gathered, the system exits the research loop and passes all findings to a CitationAgent, which processes the documents and research report to identify specific locations for citations. This ensures all claims are properly attributed to their sources. The final research results, complete with citations, are then returned to the user.\\nPrompt engineering and evaluations for research agents\\nMulti-agent systems have key differences from single-agent systems, including a rapid growth in coordination complexity. Early agents made errors like spawning 50 subagents for simple queries, scouring the web endlessly for nonexistent sources, and distracting each other with excessive updates. Since each agent is steered by a prompt, prompt engineering was our primary lever for improving these behaviors. Below are some principles we learned for prompting agents:\\nThink like your agents.\\nTo iterate on prompts, you must understand their effects. To help us do this, we built simulations using our\\nConsole\\nwith the exact prompts and tools from our system, then watched agents work step-by-step. This immediately revealed failure modes: agents continuing when they already had sufficient results, using overly verbose search queries, or selecting incorrect tools. Effective prompting relies on developing an accurate mental model of the agent, which can make the most impactful changes obvious.\\nTeach the orchestrator how to delegate.\\nIn our system, the lead agent decomposes queries into subtasks and describes them to subagents. Each subagent needs an objective, an output format, guidance on the tools and sources to use, and clear task boundaries. Without detailed task descriptions, agents duplicate work, leave gaps, or fail to find necessary information. We started by allowing the lead agent to give simple, short instructions like 'research the semiconductor shortage,' but found these instructions often were vague enough that subagents misinterpreted the task or performed the exact same searches as other agents. For instance, one subagent explored the 2021 automotive chip crisis while 2 others duplicated work investigating current 2025 supply chains, without an effective division of labor.\\nScale effort to query complexity.\\nAgents struggle to judge appropriate effort for different tasks, so we embedded scaling rules in the prompts. Simple fact-finding requires just 1 agent with 3-10 tool calls, direct comparisons might need 2-4 subagents with 10-15 calls each, and complex research might use more than 10 subagents with clearly divided responsibilities. These explicit guidelines help the lead agent allocate resources efficiently and prevent overinvestment in simple queries, which was a common failure mode in our early versions.\\nTool design and selection are critical.\\nAgent-tool interfaces are as critical as human-computer interfaces. Using the right tool is efficient—often, it’s strictly necessary. For instance, an agent searching the web for context that only exists in Slack is doomed from the start. With\\nMCP servers\\nthat give the model access to external tools, this problem compounds, as agents encounter unseen tools with descriptions of wildly varying quality. We gave our agents explicit heuristics: for example, examine all available tools first, match tool usage to user intent, search the web for broad external exploration, or prefer specialized tools over generic ones. Bad tool descriptions can send agents down completely wrong paths, so each tool needs a distinct purpose and a clear description.\\nLet agents improve themselves\\n. We found that the Claude 4 models can be excellent prompt engineers. When given a prompt and a failure mode, they are able to diagnose why the agent is failing and suggest improvements. We even created a tool-testing agent—when given a flawed MCP tool, it attempts to use the tool and then rewrites the tool description to avoid failures. By testing the tool dozens of times, this agent found key nuances and bugs. This process for improving tool ergonomics resulted in a 40% decrease in task completion time for future agents using the new description, because they were able to avoid most mistakes.\\nStart wide, then narrow down.\\nSearch strategy should mirror expert human research: explore the landscape before drilling into specifics. Agents often default to overly long, specific queries that return few results. We counteracted this tendency by prompting agents to start with short, broad queries, evaluate what’s available, then progressively narrow focus.\\nGuide the thinking process.\\nExtended thinking mode\\n, which leads Claude to output additional tokens in a visible thinking process, can serve as a controllable scratchpad. The lead agent uses thinking to plan its approach, assessing which tools fit the task, determining query complexity and subagent count, and defining each subagent’s role. Our testing showed that extended thinking improved instruction-following, reasoning, and efficiency. Subagents also plan, then use\\ninterleaved thinking\\nafter tool results to evaluate quality, identify gaps, and refine their next query. This makes subagents more effective in adapting to any task.\\nParallel tool calling transforms speed and performance.\\nComplex research tasks naturally involve exploring many sources. Our early agents executed sequential searches, which was painfully slow. For speed, we introduced two kinds of parallelization: (1) the lead agent spins up 3-5 subagents in parallel rather than serially; (2) the subagents use 3+ tools in parallel. These changes cut research time by up to 90% for complex queries, allowing Research to do more work in minutes instead of hours while covering more information than other systems.\\nOur prompting strategy focuses on instilling good heuristics rather than rigid rules. We studied how skilled humans approach research tasks and encoded these strategies in our prompts—strategies like decomposing difficult questions into smaller tasks, carefully evaluating the quality of sources, adjusting search approaches based on new information, and recognizing when to focus on depth (investigating one topic in detail) vs. breadth (exploring many topics in parallel). We also proactively mitigated unintended side effects by setting explicit guardrails to prevent the agents from spiraling out of control. Finally, we focused on a fast iteration loop with observability and test cases.\\nEffective evaluation of agents\\nGood evaluations are essential for building reliable AI applications, and agents are no different. However, evaluating multi-agent systems presents unique challenges. Traditional evaluations often assume that the AI follows the same steps each time: given input X, the system should follow path Y to produce output Z. But multi-agent systems don't work this way. Even with identical starting points, agents might take completely different valid paths to reach their goal. One agent might search three sources while another searches ten, or they might use different tools to find the same answer. Because we don’t always know what the right steps are, we usually can't just check if agents followed the “correct” steps we prescribed in advance. Instead, we need flexible evaluation methods that judge whether agents achieved the right outcomes while also following a reasonable process.\\nStart evaluating immediately with small samples\\n. In early agent development, changes tend to have dramatic impacts because there is abundant low-hanging fruit. A prompt tweak might boost success rates from 30% to 80%. With effect sizes this large, you can spot changes with just a few test cases. We started with a set of about 20 queries representing real usage patterns. Testing these queries often allowed us to clearly see the impact of changes. We often hear that AI developer teams delay creating evals because they believe that only large evals with hundreds of test cases are useful. However, it’s best to start with small-scale testing right away with a few examples, rather than delaying until you can build more thorough evals.\\nLLM-as-judge evaluation scales when done well.\\nResearch outputs are difficult to evaluate programmatically, since they are free-form text and rarely have a single correct answer. LLMs are a natural fit for grading outputs. We used an LLM judge that evaluated each output against criteria in a rubric: factual accuracy (do claims match sources?), citation accuracy (do the cited sources match the claims?), completeness (are all requested aspects covered?), source quality (did it use primary sources over lower-quality secondary sources?), and tool efficiency (did it use the right tools a reasonable number of times?). We experimented with multiple judges to evaluate each component, but found that a single LLM call with a single prompt outputting scores from 0.0-1.0 and a pass-fail grade was the most consistent and aligned with human judgements. This method was especially effective when the eval test cases\\ndid\\nhave a clear answer, and we could use the LLM judge to simply check if the answer was correct (i.e. did it accurately list the pharma companies with the top 3 largest R&D budgets?). Using an LLM as a judge allowed us to scalably evaluate hundreds of outputs.\\nHuman evaluation catches what automation misses.\\nPeople testing agents find edge cases that evals miss. These include hallucinated answers on unusual queries, system failures, or subtle source selection biases. In our case, human testers noticed that our early agents consistently chose SEO-optimized content farms over authoritative but less highly-ranked sources like academic PDFs or personal blogs. Adding source quality heuristics to our prompts helped resolve this issue. Even in a world of automated evaluations, manual testing remains essential.\\nMulti-agent systems have emergent behaviors, which arise without specific programming. For instance, small changes to the lead agent can unpredictably change how subagents behave. Success requires understanding interaction patterns, not just individual agent behavior. Therefore, the best prompts for these agents are not just strict instructions, but frameworks for collaboration that define the division of labor, problem-solving approaches, and effort budgets. Getting this right relies on careful prompting and tool design, solid heuristics, observability, and tight feedback loops.\\nSee the\\nopen-source prompts in our Cookbook\\nfor example prompts from our system.\\nProduction reliability and engineering challenges\\nIn traditional software, a bug might break a feature, degrade performance, or cause outages. In agentic systems, minor changes cascade into large behavioral changes, which makes it remarkably difficult to write code for complex agents that must maintain state in a long-running process.\\nAgents are stateful and errors compound.\\nAgents can run for long periods of time, maintaining state across many tool calls. This means we need to durably execute code and handle errors along the way. Without effective mitigations, minor system failures can be catastrophic for agents. When errors occur, we can't just restart from the beginning: restarts are expensive and frustrating for users. Instead, we built systems that can resume from where the agent was when the errors occurred. We also use the model’s intelligence to handle issues gracefully: for instance, letting the agent know when a tool is failing and letting it adapt works surprisingly well. We combine the adaptability of AI agents built on Claude with deterministic safeguards like retry logic and regular checkpoints.\\nDebugging benefits from new approaches.\\nAgents make dynamic decisions and are non-deterministic between runs, even with identical prompts. This makes debugging harder. For instance, users would report agents “not finding obvious information,” but we couldn't see why. Were the agents using bad search queries? Choosing poor sources? Hitting tool failures? Adding full production tracing let us diagnose why agents failed and fix issues systematically. Beyond standard observability, we monitor agent decision patterns and interaction structures—all without monitoring the contents of individual conversations, to maintain user privacy. This high-level observability helped us diagnose root causes, discover unexpected behaviors, and fix common failures.\\nDeployment needs careful coordination.\\nAgent systems are highly stateful webs of prompts, tools, and execution logic that run almost continuously. This means that whenever we deploy updates, agents might be anywhere in their process. We therefore need to prevent our well-meaning code changes from breaking existing agents. We can’t update every agent to the new version at the same time. Instead, we use\\nrainbow deployments\\nto avoid disrupting running agents, by gradually shifting traffic from old to new versions while keeping both running simultaneously.\\nSynchronous execution creates bottlenecks.\\nCurrently, our lead agents execute subagents synchronously, waiting for each set of subagents to complete before proceeding. This simplifies coordination, but creates bottlenecks in the information flow between agents. For instance, the lead agent can’t steer subagents, subagents can’t coordinate, and the entire system can be blocked while waiting for a single subagent to finish searching. Asynchronous execution would enable additional parallelism: agents working concurrently and creating new subagents when needed. But this asynchronicity adds challenges in result coordination, state consistency, and error propagation across the subagents. As models can handle longer and more complex research tasks, we expect the performance gains will justify the complexity.\\nConclusion\\nWhen building AI agents, the last mile often becomes most of the journey. Codebases that work on developer machines require significant engineering to become reliable production systems. The compound nature of errors in agentic systems means that minor issues for traditional software can derail agents entirely. One step failing can cause agents to explore entirely different trajectories, leading to unpredictable outcomes. For all the reasons described in this post, the gap between prototype and production is often wider than anticipated.\\nDespite these challenges, multi-agent systems have proven valuable for open-ended research tasks. Users have said that Claude helped them find business opportunities they hadn’t considered, navigate complex healthcare options, resolve thorny technical bugs, and save up to days of work by uncovering research connections they wouldn't have found alone. Multi-agent research systems can operate reliably at scale with careful engineering, comprehensive testing, detail-oriented prompt and tool design, robust operational practices, and tight collaboration between research, product, and engineering teams who have a strong understanding of current agent capabilities. We're already seeing these systems transform how people solve complex problems.\\nA\\nClio\\nembedding plot showing the most common ways people are using the Research feature today. The top use case categories are developing software systems across specialized domains (10%), develop and optimize professional and technical content (8%), develop business growth and revenue generation strategies (8%), assist with academic research and educational material development (7%), and research and verify information about people, places, or organizations (5%).\\nAcknowlegements\\nWritten by Jeremy Hadfield, Barry Zhang, Kenneth Lien, Florian Scholz, Jeremy Fox, and Daniel Ford. This work reflects the collective efforts of several teams across Anthropic who made the Research feature possible. Special thanks go to the Anthropic apps engineering team, whose dedication brought this complex multi-agent system to production. We're also grateful to our early users for their excellent feedback.\\nAppendix\\nBelow are some additional miscellaneous tips for multi-agent systems.\\nEnd-state evaluation of agents that mutate state over many turns.\\nEvaluating agents that modify persistent state across multi-turn conversations presents unique challenges. Unlike read-only research tasks, each action can change the environment for subsequent steps, creating dependencies that traditional evaluation methods struggle to handle. We found success focusing on end-state evaluation rather than turn-by-turn analysis. Instead of judging whether the agent followed a specific process, evaluate whether it achieved the correct final state. This approach acknowledges that agents may find alternative paths to the same goal while still ensuring they deliver the intended outcome. For complex workflows, break evaluation into discrete checkpoints where specific state changes should have occurred, rather than attempting to validate every intermediate step.\\nLong-horizon conversation management.\\nProduction agents often engage in conversations spanning hundreds of turns, requiring careful context management strategies. As conversations extend, standard context windows become insufficient, necessitating intelligent compression and memory mechanisms. We implemented patterns where agents summarize completed work phases and store essential information in external memory before proceeding to new tasks. When context limits approach, agents can spawn fresh subagents with clean contexts while maintaining continuity through careful handoffs. Further, they can retrieve stored context like the research plan from their memory rather than losing previous work when reaching the context limit. This distributed approach prevents context overflow while preserving conversation coherence across extended interactions.\\nSubagent output to a filesystem to minimize the ‘game of telephone.’\\nDirect subagent outputs can bypass the main coordinator for certain types of results, improving both fidelity and performance. Rather than requiring subagents to communicate everything through the lead agent, implement artifact systems where specialized agents can create outputs that persist independently. Subagents call tools to store their work in external systems, then pass lightweight references back to the coordinator. This prevents information loss during multi-stage processing and reduces token overhead from copying large outputs through conversation history. The pattern works particularly well for structured outputs like code, reports, or data visualizations where the subagent's specialized prompt produces better results than filtering through a general coordinator.\\nProduct\\nClaude overview\\nClaude Code\\nMax plan\\nTeam plan\\nEnterprise plan\\nDownload Claude apps\\nClaude.ai pricing plans\\nClaude.ai login\\nAPI Platform\\nAPI overview\\nDeveloper docs\\nClaude in Amazon Bedrock\\nClaude on Google Cloud's Vertex AI\\nPricing\\nConsole login\\nResearch\\nResearch overview\\nEconomic Index\\nClaude models\\nClaude Opus 4\\nClaude Sonnet 4\\nClaude Haiku 3.5\\nCommitments\\nTransparency\\nResponsible scaling policy\\nSecurity and compliance\\nSolutions\\nAI agents\\nCoding\\nCustomer support\\nEducation\\nFinancial services\\nLearn\\nAnthropic Academy\\nCustomer stories\\nEngineering at Anthropic\\nMCP Integrations\\nPartner Directory\\nExplore\\nAbout us\\nBecome a partner\\nCareers\\nEvents\\nNews\\nStartups program\\nHelp and security\\nStatus\\nAvailability\\nSupport center\\nTerms and policies\\nPrivacy choices\\nPrivacy policy\\nResponsible disclosure policy\\nTerms of service - consumer\\nTerms of service - commercial\\nUsage policy\\n© 2025 Anthropic PBC\"}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def messages_for(website):\n",
    "    return[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]\n",
    "\n",
    "messages_for(blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d10b8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Summary of \"How We Built Our Multi-Agent Research System\"\n",
       "\n",
       "The article details the engineering journey of developing a multi-agent research system using multiple Claude language models (LLMs) to tackle complex research tasks more efficiently. This system allows the lead agent to orchestrate several subagents that operate concurrently, which increases performance, especially for dynamic and unpredictable research queries.\n",
       "\n",
       "## Key Highlights\n",
       "\n",
       "- **Multi-Agent System Overview**: The system employs a lead agent that plans the research process and spawns various subagents to work in parallel, improving problem-solving capabilities by allowing broader exploration without being path-dependent.\n",
       "\n",
       "- **Research Efficiency**: Internal evaluations showed that the multi-agent system significantly outperformed single-agent setups, particularly in tasks requiring extensive information retrieval.\n",
       "\n",
       "- **Token Usage and Performance**: A multi-agent system utilizes token calls more effectively, substantially improving efficiency in complex queries, although it requires managing higher token costs for economic feasibility.\n",
       "\n",
       "- **Architecture Description**: The architecture involves an orchestrator-worker model, where the lead agent creates subagents for specific tasks, showcasing a dynamic approach to information gathering compared to traditional static methods.\n",
       "\n",
       "- **Prompt Engineering**: Essential principles for effective prompting were discussed, emphasizing the importance of clear delegation and task separation for achieving optimal performance from the agents.\n",
       "\n",
       "- **Production Challenges**: The article explores various challenges faced in deploying and maintaining such systems, including error handling, state management, and the coordination of asynchronous operations.\n",
       "\n",
       "- **Human Evaluation**: While automated evaluations assist in measuring performance, human testers play a critical role in identifying edge cases and potential biases in agent behavior.\n",
       "\n",
       "## Usage Insights\n",
       "\n",
       "Anthropic's research feature is utilized across various domains, such as software development, content optimization, business strategy, and academic research, reflecting its versatility in handling complex, open-ended tasks.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "The engineering process highlighted the gap between prototype and production, emphasizing the extensive work needed to create reliable multi-agent systems capable of significant problem-solving in research contexts. Regular feedback loops and collaborative efforts are central to refining these systems for scaling and reliability.\n",
       "\n",
       "This article was published on June 13, 2025, and reflects contributions from multiple team members at Anthropic."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages=messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def dispaly_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))\n",
    "\n",
    "dispaly_summary(\"https://www.anthropic.com/engineering/built-multi-agent-research-system?utm_source=alphasignal&utm_campaign=2025-07-18&asuniq=02f5e591\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b45481",
   "metadata": {},
   "source": [
    "### selenium web scapping for sites which use js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2b1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs chrome webdriver (https://sites.google.com/chromium.org/driver/)\n",
    "import undetected_chromedriver as uc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By # select elements\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "# service = Service(executable_path=\"./chromedriver\")\n",
    "# driver = webdriver.Chrome(service=service) \n",
    "\n",
    "driver = uc.Chrome(headless=False)\n",
    "\n",
    "driver.get(\"https://openai.com/index/introducing-chatgpt-agent/\")\n",
    "time.sleep(5)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from openai import OpenAI\n",
    "\n",
    "class BRK_Scaper:\n",
    "    def __init__(self, url, model_provider, model_version=\"deepseek-r1:8b\", ):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using BeautifulSoupe Library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.driver = uc.Chrome(headless=False)\n",
    "        self.model_version = model_version\n",
    "        self.model_provider = model_provider\n",
    "\n",
    "    def get_page(self):\n",
    "        self.driver.get(self.url)\n",
    "        html = self.driver.page_source\n",
    "        self.driver.quit()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelavent in soup.body([\"script\", \"style\", \"img\", \"input\", \"nav\", \"style\", ]):\n",
    "            irrelavent.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        # self.soup = soup (for debugging or selectively retaining page contents)\n",
    "\n",
    "    \n",
    "        \n",
    "    def get_summary(self):\n",
    "        \n",
    "        if self.model_provider=='ollama':\n",
    "            llm = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "        elif self.model_provider=='openai':\n",
    "            llm = OpenAI()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model provider: {self.model_provider}\")\n",
    "\n",
    "        self.system_prompt = \"You are an assistant that analyses the contents of a website \\\n",
    "            and provides a short summary, ignoring text that might be navigation related. \\\n",
    "            Respond in markdown.\"\n",
    "        \n",
    "        self.human_prompt = f\"You are looking at a website titled: {self.title} \\\n",
    "            please provide a short summary of this website in markdown.\\\n",
    "            if it includes news or announcements, then summarize these too. \\n\\n \\\n",
    "            {self.text}\" \n",
    "\n",
    "        response = llm.chat.completions.create(\n",
    "            model=self.model_version,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": self.human_prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.summary = response.choices[0].message.content\n",
    "        display(Markdown(self.summary))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28a4fa39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Summary of \"Introducing ChatGPT agent: bridging research and action | OpenAI\"\n",
       "\n",
       "On July 17, 2025, OpenAI announced the launch of the **ChatGPT agent**, a new feature that enhances ChatGPT's capabilities to think and act proactively, utilizing its own computer to perform complex tasks. Users can now delegate tasks such as analyzing calendars, planning meals, or creating presentations, with ChatGPT intelligently navigating the web and handling workflows autonomously. \n",
       "\n",
       "### Key Features:\n",
       "- **Unified Agentic System**: Combines strengths from previous models (Operator for web interaction and deep research for information synthesis) to perform tasks efficiently.\n",
       "- **Tool Suite**: Includes a visual browser and text-based browser for web interactions, terminal access, and API integration.\n",
       "- **Iterative Workflows**: Users can interrupt tasks and provide additional directions as needed, allowing for a more collaborative experience.\n",
       "- **Real-World Utility**: Useful for both personal and professional tasks, such as scheduling appointments, planning travel, or automating reports.\n",
       "- **Safety Measures**: Enhanced controls for data privacy and task confirmations to reduce risks associated with accessing live web data.\n",
       "\n",
       "### Performance Highlights:\n",
       "- Achieved a state-of-the-art performance on various evaluation benchmarks, such as completing complex real-world tasks and improving on previous models.\n",
       "  \n",
       "### Availability:\n",
       "The ChatGPT agent starts rolling out to Pro, Plus, and Team users, with access expanding to other user tiers in the coming weeks.\n",
       "\n",
       "### Risks and Controls:\n",
       "OpenAI outlined potential risks, particularly regarding data usage and web interactions, implementing action confirmations and user oversight to mitigate these concerns. \n",
       "\n",
       "### Future Enhancements:\n",
       "OpenAI plans to continuously improve the ChatGPT agent's functionality, expanding its capabilities while ensuring user safety.\n",
       "\n",
       "This launch marks the beginning of an evolving interaction model, where users can expect enhanced intelligence and automation in handling tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://openai.com/index/introducing-chatgpt-agent/\"\n",
    "scrapper = BRK_Scaper(url=url, model_provider='openai', model_version=\"gpt-4o-mini\")\n",
    "scrapper.get_page()\n",
    "scrapper.get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73f77fa",
   "metadata": {},
   "source": [
    "### Streaming in jupiter  \n",
    "##### Code-copilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26977de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ###  Explanation  of  the  Code \n",
      "\n",
      " The  given  code  snippet  is  using  a  generator  expression  to  yield  values .  Here's  a  breakdown :\n",
      "\n",
      " 1 .  ** Generator  Expression ** :  \n",
      "     -  The  code  is  creating  a  generator  that  will  yield  a  set  of  unique  authors .\n",
      "\n",
      " 2 .  ** Set  Com preh ension ** :  \n",
      "     -  The  syntax  `{ ...  for  ...  in  ... }`  indicates  that  a  set  is  being  constructed .\n",
      "     -  This  set  will  contain  values  returned  by  ` book .get (\" author \") `.\n",
      "\n",
      " 3 .  ** Iteration ** :\n",
      "     -  It  iter ates  over  ` books `,  which  is  presumably  a  list  or  collection  of  ` book `  dictionaries .\n",
      "\n",
      " 4 .  ** Condition ** :\n",
      "     -  The  ` if  book .get (\" author \") `  condition  filters  out  any  ` book `  entries  that  do  not  have  an  `\" author \"`  key  or  where  the  value  is  ` None `  ( or  evaluates  to  False ).\n",
      "\n",
      " ###  Summary \n",
      "\n",
      " This  code  yields  a  set  of  authors  from  a  collection  of  book  dictionaries ,  including  only  those  books  that  have  a  defined  author . None "
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "code_to_explain = 'yield{book.get(\"author\") for book in books if book.get(\"author\")}'\n",
    "system_prompt = \"You are a helpful AI agent who explains given code in clear markdown format.\"\n",
    "user_prompt = f\"Explain below code to me. Provide your responce in short and clear markdown format. \\n\\n code \\n\\n {code_to_explain}\"\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  print(chunk.choices[0].delta.content, end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "716b9788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waves crash, salt disperses,\n",
      "Oceans hold the world's verses.\n",
      "Tides ebb and flow, ever-changing,\n",
      "Saltiness, a story ever-engaging.\n"
     ]
    }
   ],
   "source": [
    "client = anthropic.Anthropic()\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=\"You are a world-class poet. Respond only with short poems.\", #sys msg is seperate attribute\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Why is the ocean salty?\"}\n",
    "    ]\n",
    ")\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e2ab5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waves crashing, salt dispersing,\n",
      "Oceans vast, their depths conversing.\n",
      "Minerals leached, from land to sea,\n",
      "Saltiness, nature's mystery."
     ]
    }
   ],
   "source": [
    "result = client.messages.stream(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=\"You are a world-class poet. Respond only with short poems.\", #sys msg is seperate attribute\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Why is the ocean salty?\"}\n",
    "    ]\n",
    ")\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5226e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns from data to make predictions or decisions.\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite-preview-06-17\",\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    "    # config=genai.types.GenerateContentConfig(\n",
    "    #     thinking_config=genai.types.ThinkingConfig(thinking_budget=0) # Disables thinking,otherwise on by default\n",
    "    # ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0140198",
   "metadata": {},
   "source": [
    "### Adversarial chat between `Claude` and `openai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7dcf1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenaiModel:\n",
    "    user_task: str\n",
    "\n",
    "    def __init__(self, user_task):\n",
    "        self.system_message = \"You are a world class fullstack web developer. You carefully plan your development steps \\\n",
    "            and good at refactoring your code based on feedback.\"\n",
    "        self.human_message = user_task\n",
    "        self.messages=[\n",
    "            {\"role\": \"system\", \"content\": self.system_message},\n",
    "            {\"role\": \"user\", \"content\": self.human_message}\n",
    "        ]\n",
    "        self.responses=[]\n",
    "\n",
    "    def generate_response(self):\n",
    "        client = OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"o4-mini\",\n",
    "            messages=self.messages\n",
    "        )\n",
    "        self.responses.append(response.choices[0].message.content)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def incorporate_feedback(self, feedback):\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": self.responses[-1]})  # previous code\n",
    "        self.messages.append({\"role\": \"user\", \"content\": f\"Here is the review feedback:\\n{feedback}\\n\\\n",
    "                              Please revise the code accordingly.\"})\n",
    "\n",
    "class ClaudeModel:\n",
    "    user_task:str\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = anthropic.Anthropic()\n",
    "        self.system_message = \"You are a senior engineer and you do code reviews. \\\n",
    "            You analyse codebase for design principles and code correctness to provide feedback on the code. \\\n",
    "            You would be given code and your job is to analyse the provided code and provide feedback.\"\n",
    "        # self.human_message = f\"Please review this code and provide your feedback. \\n\\n code:\\n{user_task}\"\n",
    "        # self.messages=[\n",
    "        #     {\"role\": \"user\", \"content\": self.human_message}\n",
    "        # ]\n",
    "        # self.responces=[]\n",
    "\n",
    "    def generate_feedback(self, code):\n",
    "        client = anthropic.Anthropic()\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-haiku-latest\",\n",
    "            max_tokens=8192,\n",
    "            temperature=0.7,\n",
    "            system=self.system_message,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Please review this code and provide your feedback:\\n\\n{code}\"}\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "        return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33da1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iteration 1 ---\n",
      "✅ OpenAI's code:\n",
      "Here’s a single‐page, pure HTML/CSS/JS tic-tac-toe app. It tracks a best-of-5 match, alternates who starts each round (human starts round 1), keeps human/computer scores, and offers “Next Round” and “Reset Match” controls. The computer uses minimax for perfect play.\n",
      "\n",
      "Save this as `index.html` and op\n",
      "\n",
      "....................\n",
      "🧐 Claude's feedback:\n",
      "\n",
      "Here's a comprehensive code review of the Tic-Tac-Toe implementation:\n",
      "\n",
      "### Strengths of the Implementation\n",
      "\n",
      "1. **Modular and Self-Contained Design**\n",
      "   - Single HTML file with embedded CSS and JavaScript\n",
      "   - Immediately Invoked Function Expression (IIFE) for encapsulation\n",
      "   - Clean separation of c\n",
      "\n",
      "................\n",
      "\n",
      "--- Iteration 2 ---\n",
      "✅ OpenAI's code:\n",
      "Below is a refactored single-page Tic-Tac-Toe app incorporating the review feedback:\n",
      "\n",
      "Key improvements  \n",
      "• CONFIG & GameState enums for easy tuning   \n",
      "• Alpha-beta-pruned minimax with depth to prefer quick wins   \n",
      "• More comments and strict const/let usage   \n",
      "• Basic error-handling (try/catch)  \n",
      "• A\n",
      "\n",
      "....................\n",
      "🧐 Claude's feedback:\n",
      "\n",
      "Code Review Feedback:\n",
      "\n",
      "Strengths:\n",
      "1. **Excellent Modular Design**\n",
      "   - Well-structured single-page application\n",
      "   - Encapsulated in an IIFE to prevent global scope pollution\n",
      "   - Clear separation of concerns (initialization, game logic, UI)\n",
      "\n",
      "2. **Configuration & Extensibility**\n",
      "   - `CONFIG` object \n",
      "\n",
      "................\n",
      "\n",
      "--- Iteration 3 ---\n",
      "✅ OpenAI's code:\n",
      "Below is an updated version of the single-page Tic-Tac-Toe app incorporating your latest feedback:\n",
      "\n",
      "Key Enhancements  \n",
      "• Memoization & depth-limit for minimax  \n",
      "• Extracted terminal-state check & move-generation helpers  \n",
      "• Full JSDoc on minimax and helpers  \n",
      "• CONFIG validation on init  \n",
      "• Global e\n",
      "\n",
      "....................\n",
      "🧐 Claude's feedback:\n",
      "\n",
      "Code Review Feedback:\n",
      "\n",
      "Overall Strengths:\n",
      "✅ Excellent implementation of a single-page Tic-Tac-Toe game\n",
      "✅ Strong focus on code organization and modularity\n",
      "✅ Comprehensive accessibility considerations\n",
      "✅ Robust error handling\n",
      "✅ Advanced AI implementation with minimax algorithm\n",
      "\n",
      "Specific Positive Observ\n",
      "\n",
      "................\n"
     ]
    }
   ],
   "source": [
    "user_task = \"Write a full functional single page web application to play human vs computer \\\n",
    "    tic tac. Should be just HTML, CSS and JS. Should track computer and human scores. \\\n",
    "        should allow human to reset the game. Computer and human should take turns to strat the game \\\n",
    "            with human starting first in best of 5 game match.\"\n",
    "\n",
    "# Initialize models\n",
    "openai_model = OpenaiModel(user_task)\n",
    "claude_model = ClaudeModel()\n",
    "\n",
    "# Feedback loop\n",
    "for i in range(3):  \n",
    "    print(f\"\\n--- Iteration {i+1} ---\")\n",
    "    code = openai_model.generate_response()\n",
    "    print(f\"✅ OpenAI's code:\\n{code}\\n\\n....................\")  # Truncated for readability\n",
    "\n",
    "    feedback = claude_model.generate_feedback(code)\n",
    "    print(f\"🧐 Claude's feedback:\\n\\n{feedback}\\n\\n................\")  # Truncated for readability\n",
    "\n",
    "    openai_model.incorporate_feedback(feedback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7836ce7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a world class fullstack web developer. You carefully plan your development steps             and good at refactoring your code based on feedback.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Write a full functional single page web application to play human vs computer     tic tac. Should be just HTML, CSS and JS. Should track computer and human scores.         should allow human to reset the game. Computer and human should take turns to strat the game             with human starting first in best of 5 game match.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Here’s a single‐page, pure HTML/CSS/JS tic-tac-toe app. It tracks a best-of-5 match, alternates who starts each round (human starts round 1), keeps human/computer scores, and offers “Next Round” and “Reset Match” controls. The computer uses minimax for perfect play.\\n\\nSave this as `index.html` and open in your browser.\\n\\n```html\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n  <meta charset=\"UTF-8\">\\n  <title>Tic-Tac-Toe: Human vs Computer</title>\\n  <style>\\n    body {\\n      font-family: sans-serif;\\n      text-align: center;\\n      margin: 0;\\n      padding: 20px;\\n      background: #f0f0f0;\\n    }\\n    #scoreboard {\\n      margin-bottom: 10px;\\n    }\\n    #board {\\n      display: grid;\\n      grid-template: repeat(3, 100px) / repeat(3, 100px);\\n      gap: 5px;\\n      margin: 0 auto 10px;\\n    }\\n    .cell {\\n      background: #fff;\\n      font-size: 2.5em;\\n      display: flex;\\n      align-items: center;\\n      justify-content: center;\\n      cursor: pointer;\\n      user-select: none;\\n    }\\n    .cell.disabled {\\n      cursor: default;\\n      color: #666;\\n    }\\n    #message {\\n      min-height: 1.5em;\\n      margin-bottom: 10px;\\n      font-weight: bold;\\n    }\\n    button {\\n      padding: 8px 16px;\\n      margin: 0 5px;\\n      font-size: 1em;\\n    }\\n    button:disabled {\\n      opacity: 0.5;\\n      cursor: default;\\n    }\\n  </style>\\n</head>\\n<body>\\n\\n  <div id=\"scoreboard\">\\n    <div>Round <span id=\"round-num\">1</span> of 5</div>\\n    <div>Human: <span id=\"human-score\">0</span>  –  Computer: <span id=\"computer-score\">0</span></div>\\n  </div>\\n  <div id=\"message\"></div>\\n  <div id=\"board\"></div>\\n  <button id=\"next-round\" disabled>Next Round</button>\\n  <button id=\"reset-match\">Reset Match</button>\\n\\n<script>\\n(function(){\\n  const HUMAN = \\'X\\', CPU = \\'O\\';\\n  let board, humanScore, cpuScore, roundNum, currentPlayer, gameOver;\\n  const cells = [];\\n\\n  // Cached DOM\\n  const boardEl = document.getElementById(\\'board\\');\\n  const msgEl = document.getElementById(\\'message\\');\\n  const humanScoreEl = document.getElementById(\\'human-score\\');\\n  const cpuScoreEl   = document.getElementById(\\'computer-score\\');\\n  const roundNumEl   = document.getElementById(\\'round-num\\');\\n  const nextBtn      = document.getElementById(\\'next-round\\');\\n  const resetBtn     = document.getElementById(\\'reset-match\\');\\n\\n  // Initialize cells\\n  function buildBoard(){\\n    boardEl.innerHTML = \\'\\';\\n    for(let i=0;i<9;i++){\\n      const div = document.createElement(\\'div\\');\\n      div.className = \\'cell\\';\\n      div.dataset.idx = i;\\n      div.addEventListener(\\'click\\', onCellClick);\\n      boardEl.appendChild(div);\\n      cells[i] = div;\\n    }\\n  }\\n\\n  // Start or restart match\\n  function resetMatch(){\\n    humanScore = 0; cpuScore = 0; roundNum = 1;\\n    updateScores();\\n    roundNumEl.textContent = roundNum;\\n    nextBtn.disabled = true;\\n    msgEl.textContent = \\'\\';\\n    startRound();\\n  }\\n\\n  // Start a new round\\n  function startRound(){\\n    board = Array(9).fill(null);\\n    gameOver = false;\\n    cells.forEach(c => {\\n      c.textContent = \\'\\';\\n      c.classList.remove(\\'disabled\\');\\n    });\\n    roundNumEl.textContent = roundNum;\\n    nextBtn.disabled = true;\\n    // Decide who starts: human on odd rounds, CPU on even\\n    currentPlayer = (roundNum % 2 === 1) ? HUMAN : CPU;\\n    setMessage(currentPlayer === HUMAN ? \"Your turn\" : \"Computer\\'s turn\");\\n    if(currentPlayer === CPU){\\n      setTimeout(cpuMove, 500);\\n    }\\n  }\\n\\n  // Handle click on a cell\\n  function onCellClick(e){\\n    const idx = +e.target.dataset.idx;\\n    if(gameOver || board[idx] || currentPlayer !== HUMAN) return;\\n    makeMove(idx, HUMAN);\\n    if(!gameOver) {\\n      setTimeout(cpuMove, 300);\\n    }\\n  }\\n\\n  // Place a move and check game status\\n  function makeMove(idx, player){\\n    board[idx] = player;\\n    cells[idx].textContent = player;\\n    cells[idx].classList.add(\\'disabled\\');\\n    const winner = checkWinner(board);\\n    if(winner){\\n      endRound(winner);\\n    } else if(board.every(x => x)){\\n      endRound(\\'draw\\');\\n    } else {\\n      currentPlayer = (player === HUMAN ? CPU : HUMAN);\\n      setMessage(currentPlayer === HUMAN ? \"Your turn\" : \"Computer\\'s turn\");\\n    }\\n  }\\n\\n  // End of round\\n  function endRound(result){\\n    gameOver = true;\\n    if(result === HUMAN){\\n      humanScore++;\\n      setMessage(\"You win this round!\");\\n    } else if(result === CPU){\\n      cpuScore++;\\n      setMessage(\"Computer wins this round!\");\\n    } else {\\n      setMessage(\"It\\'s a draw.\");\\n    }\\n    updateScores();\\n    // Check if match over\\n    if(humanScore === 3 || cpuScore === 3 || roundNum === 5){\\n      const champ = humanScore > cpuScore ? \"You win the match!\" : humanScore < cpuScore ? \"Computer wins the match!\" : \"Match is a tie!\";\\n      setTimeout(()=> setMessage(champ), 500);\\n      nextBtn.disabled = true;\\n    } else {\\n      nextBtn.disabled = false;\\n    }\\n    roundNum++;\\n  }\\n\\n  // Update score display\\n  function updateScores(){\\n    humanScoreEl.textContent = humanScore;\\n    cpuScoreEl.textContent   = cpuScore;\\n  }\\n\\n  // Show a message\\n  function setMessage(txt){\\n    msgEl.textContent = txt;\\n  }\\n\\n  // Computer move via minimax\\n  function cpuMove(){\\n    if(gameOver) return;\\n    const best = minimax(board, CPU).idx;\\n    makeMove(best, CPU);\\n  }\\n\\n  // Minimax algorithm\\n  function minimax(pos, player){\\n    const avail = pos.map((v,i)=>v?null:i).filter(v=>v!==null);\\n    const winner = checkWinner(pos);\\n    if(winner === HUMAN) return {score: -10};\\n    if(winner === CPU)   return {score: 10};\\n    if(avail.length === 0) return {score: 0};\\n\\n    const moves = [];\\n    for(const idx of avail){\\n      const copy = pos.slice();\\n      copy[idx] = player;\\n      const result = minimax(copy, player===CPU ? HUMAN : CPU);\\n      moves.push({idx, score: result.score});\\n    }\\n    let bestMove;\\n    if(player === CPU){\\n      let max = -Infinity;\\n      moves.forEach(m => { if(m.score>max){ max=m.score; bestMove=m;}});\\n    } else {\\n      let min = Infinity;\\n      moves.forEach(m => { if(m.score<min){ min=m.score; bestMove=m;}});\\n    }\\n    return bestMove;\\n  }\\n\\n  // Check for winner or return null\\n  function checkWinner(b){\\n    const lines = [\\n      [0,1,2],[3,4,5],[6,7,8],\\n      [0,3,6],[1,4,7],[2,5,8],\\n      [0,4,8],[2,4,6]\\n    ];\\n    for(const [a,b1,c] of lines){\\n      if(b[a] && b[a] === b[b1] && b[a] === b[c]){\\n        return b[a];\\n      }\\n    }\\n    return null;\\n  }\\n\\n  // Event wiring\\n  nextBtn.addEventListener(\\'click\\', startRound);\\n  resetBtn.addEventListener(\\'click\\', resetMatch);\\n\\n  // boot\\n  buildBoard();\\n  resetMatch();\\n\\n})();\\n</script>\\n\\n</body>\\n</html>\\n```\\n\\nHow it works:\\n 1. The page builds a 3×3 grid of clickable cells.\\n 2. You play “X,” the computer plays “O.”  \\n 3. On odd rounds (1,3,5) you start; on even rounds the AI starts.  \\n 4. After each round ends (win/lose/draw), scores update and the “Next Round” button enables (unless the match is over).  \\n 5. First to 3 wins—or after 5 rounds—the match concludes and you can “Reset Match” to play again.  \\n\\nFeel free to tweak styles or timing delays to your taste!'},\n",
       " {'role': 'user',\n",
       "  'content': \"Here is the review feedback:\\nHere's a comprehensive code review of the Tic-Tac-Toe implementation:\\n\\n### Strengths of the Implementation\\n\\n1. **Modular and Self-Contained Design**\\n   - Single HTML file with embedded CSS and JavaScript\\n   - Immediately Invoked Function Expression (IIFE) for encapsulation\\n   - Clean separation of concerns between UI, game logic, and AI\\n\\n2. **Robust Game Logic**\\n   - Implements a best-of-5 match system\\n   - Alternates starting player between rounds\\n   - Tracks and displays scores\\n   - Handles game state transitions cleanly\\n\\n3. **Minimax AI Implementation**\\n   - Implements a perfect play algorithm\\n   - Recursive minimax ensures optimal computer moves\\n   - Handles different game scenarios effectively\\n\\n4. **User Experience Considerations**\\n   - Clear UI with responsive design\\n   - Informative messages about game state\\n   - Disabled buttons and cells to prevent invalid moves\\n   - Slight delays for computer moves to feel more natural\\n\\n### Potential Improvements\\n\\n1. **Code Structure and Maintainability**\\n   - Consider breaking into separate HTML, CSS, and JS files for larger projects\\n   - Add comments explaining complex logic, especially in minimax algorithm\\n   - Use const/let more consistently (some var-like usages)\\n\\n2. **Error Handling and Validation**\\n   - Add input validation for move selections\\n   - Implement more robust error handling\\n   - Consider adding try-catch blocks for critical sections\\n\\n3. **Performance Optimization**\\n   - Minimax can be computationally expensive for larger boards\\n   - Consider alpha-beta pruning to improve minimax efficiency\\n   - Memoization could help cache intermediate minimax results\\n\\n4. **Accessibility Improvements**\\n   - Add ARIA attributes for screen readers\\n   - Ensure keyboard navigation for game board\\n   - Improve color contrast for better readability\\n\\n### Specific Code Suggestions\\n\\n1. **Minimax Function Refinement**\\n```javascript\\nfunction minimax(pos, player, depth = 0) {\\n  // Add depth parameter to limit search\\n  // Implement alpha-beta pruning\\n  // Consider adding a scoring mechanism that prefers faster wins/losses\\n}\\n```\\n\\n2. **State Management**\\n```javascript\\n// Consider using a more robust state management approach\\nconst GameState = {\\n  INITIALIZED: 'initialized',\\n  IN_PROGRESS: 'in_progress',\\n  ROUND_COMPLETE: 'round_complete',\\n  MATCH_COMPLETE: 'match_complete'\\n};\\n```\\n\\n3. **Configuration Object**\\n```javascript\\nconst CONFIG = {\\n  BOARD_SIZE: 3,\\n  MATCH_ROUNDS: 5,\\n  WINNING_SCORE: 3,\\n  PLAYERS: {\\n    HUMAN: 'X',\\n    CPU: 'O'\\n  }\\n};\\n```\\n\\n### Security Considerations\\n- Ensure no user-controllable inputs can inject malicious scripts\\n- Current implementation is client-side only, so minimal risk\\n\\n### Testing Recommendations\\n- Unit tests for minimax algorithm\\n- Integration tests for game state transitions\\n- Edge case testing (draw scenarios, match completion)\\n\\n### Overall Rating: 4/5 ⭐⭐⭐⭐\\n\\nThe implementation is solid, well-structured, and demonstrates several advanced JavaScript and game design concepts. The minimax AI provides a challenging opponent, and the game flow is intuitive.\\n\\nThe code shows a deep understanding of:\\n- Functional programming principles\\n- Game state management\\n- Algorithm implementation\\n- UI/UX design\\n\\n### Recommendation\\nThis is an excellent example of a self-contained, pure JavaScript game implementation. For production, consider:\\n- Separating concerns into modules\\n- Adding more comprehensive error handling\\n- Implementing more advanced AI strategies\\n- Adding comprehensive test coverage\\n\\nWould you like me to elaborate on any specific aspect of the review?\\n                              Please revise the code accordingly.\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Below is a refactored single-page Tic-Tac-Toe app incorporating the review feedback:\\n\\nKey improvements  \\n• CONFIG & GameState enums for easy tuning   \\n• Alpha-beta-pruned minimax with depth to prefer quick wins   \\n• More comments and strict const/let usage   \\n• Basic error-handling (try/catch)  \\n• Accessibility: ARIA roles, live region, keyboard support  \\n\\nSave as `index.html` and open in your browser.\\n\\n```html\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n  <meta charset=\"UTF-8\">\\n  <title>Tic-Tac-Toe: Human vs Computer</title>\\n  <style>\\n    /* =======================\\n         Basic Reset & Layout\\n       ======================= */\\n    * { box-sizing: border-box; margin:0; padding:0 }\\n    body {\\n      font-family: sans-serif;\\n      background: #fafafa;\\n      color: #222;\\n      text-align: center;\\n      padding: 20px;\\n    }\\n    /* ==================\\n       Scoreboard & Msg\\n       ================== */\\n    #scoreboard { margin-bottom: 10px; }\\n    #message {\\n      min-height: 1.5em;\\n      margin-bottom: 10px;\\n      font-weight: bold;\\n    }\\n    /* ==================\\n       Game Board Grid\\n       ================== */\\n    #board {\\n      display: grid;\\n      grid-template: repeat(3, 100px) / repeat(3, 100px);\\n      gap: 5px;\\n      margin: 0 auto 10px;\\n      width: max-content;\\n    }\\n    .cell {\\n      background: #fff;\\n      border: 2px solid #333;\\n      font-size: 2.5em;\\n      display: flex;\\n      align-items: center;\\n      justify-content: center;\\n      cursor: pointer;\\n      user-select: none;\\n      transition: background 0.2s;\\n    }\\n    .cell:focus { outline: 3px solid #06f; }\\n    .cell.disabled { cursor: default; color: #666; }\\n    /* ============\\n       Buttons\\n       ============ */\\n    button {\\n      padding: 8px 16px;\\n      margin: 0 5px;\\n      font-size: 1em;\\n      cursor: pointer;\\n    }\\n    button:disabled {\\n      opacity: 0.5;\\n      cursor: default;\\n    }\\n  </style>\\n</head>\\n<body>\\n\\n  <div id=\"scoreboard\">\\n    <div>Round <span id=\"round-num\">1</span> of <span id=\"total-rounds\"></span></div>\\n    <div>\\n      <span id=\"player-label\">You (<strong>X</strong>)</span>: \\n      <span id=\"human-score\">0</span>  \\n      –  \\n      <span id=\"cpu-label\">Computer (<strong>O</strong>)</span>: \\n      <span id=\"computer-score\">0</span>\\n    </div>\\n  </div>\\n\\n  <div id=\"message\" aria-live=\"polite\"></div>\\n\\n  <div id=\"board\" role=\"grid\" aria-label=\"Tic-Tac-Toe board\"></div>\\n\\n  <div>\\n    <button id=\"next-round\" disabled>Next Round</button>\\n    <button id=\"reset-match\">Reset Match</button>\\n  </div>\\n\\n<script>\\n(function(){\\n  \\'use strict\\';\\n\\n  // ===== Configuration & Enums =====\\n  const CONFIG = {\\n    BOARD_SIZE:   3,\\n    MATCH_ROUNDS: 5,\\n    WINNING_SCORE: 3,\\n    PLAYERS: { HUMAN: \\'X\\', CPU: \\'O\\' }\\n  };\\n  const GameState = {\\n    INITIALIZED:   \\'initialized\\',\\n    IN_PROGRESS:   \\'in_progress\\',\\n    ROUND_COMPLETE:\\'round_complete\\',\\n    MATCH_COMPLETE:\\'match_complete\\'\\n  };\\n\\n  // ===== App State =====\\n  let board, humanScore, cpuScore, roundNum, currentPlayer, gameState;\\n  const cells = [];\\n\\n  // ===== Cached DOM =====\\n  const boardEl     = document.getElementById(\\'board\\');\\n  const msgEl       = document.getElementById(\\'message\\');\\n  const humanScoreEl= document.getElementById(\\'human-score\\');\\n  const cpuScoreEl  = document.getElementById(\\'computer-score\\');\\n  const roundNumEl  = document.getElementById(\\'round-num\\');\\n  const totalRoundsEl = document.getElementById(\\'total-rounds\\');\\n  const nextBtn     = document.getElementById(\\'next-round\\');\\n  const resetBtn    = document.getElementById(\\'reset-match\\');\\n\\n  // ================\\n  // Initialization\\n  // ================\\n  function init(){\\n    totalRoundsEl.textContent = CONFIG.MATCH_ROUNDS;\\n    buildBoard();\\n    bindControls();\\n    resetMatch();\\n  }\\n\\n  // Build the 3×3 grid of cells, with click & keyboard support\\n  function buildBoard(){\\n    boardEl.innerHTML = \\'\\';\\n    for(let i=0; i<CONFIG.BOARD_SIZE**2; i++){\\n      const div = document.createElement(\\'div\\');\\n      div.className = \\'cell\\';\\n      div.setAttribute(\\'role\\',\\'button\\');\\n      div.setAttribute(\\'tabindex\\',\\'0\\');\\n      div.setAttribute(\\'aria-label\\', `Cell ${Math.floor(i/3)+1},${i%3+1}`);\\n      div.dataset.idx = i;\\n      div.addEventListener(\\'click\\', onCellAction);\\n      div.addEventListener(\\'keydown\\', onCellAction);\\n      boardEl.appendChild(div);\\n      cells[i] = div;\\n    }\\n  }\\n\\n  // Wire up Next & Reset buttons\\n  function bindControls(){\\n    nextBtn.addEventListener(\\'click\\', startRound);\\n    resetBtn.addEventListener(\\'click\\', resetMatch);\\n  }\\n\\n  // ================\\n  // Match Lifecycle\\n  // ================\\n  function resetMatch(){\\n    humanScore = 0;\\n    cpuScore   = 0;\\n    roundNum   = 1;\\n    gameState  = GameState.INITIALIZED;\\n    updateScores();\\n    roundNumEl.textContent = roundNum;\\n    nextBtn.disabled = true;\\n    setMessage(\\'Click \"Next Round\" to begin\\');\\n  }\\n\\n  function startRound(){\\n    try {\\n      board = Array(CONFIG.BOARD_SIZE**2).fill(null);\\n      gameState = GameState.IN_PROGRESS;\\n      cells.forEach(c => {\\n        c.textContent = \\'\\';\\n        c.classList.remove(\\'disabled\\');\\n      });\\n      roundNumEl.textContent = roundNum;\\n      nextBtn.disabled = true;\\n\\n      // Alternate start: human on odd rounds\\n      currentPlayer = (roundNum % 2 === 1)\\n                      ? CONFIG.PLAYERS.HUMAN\\n                      : CONFIG.PLAYERS.CPU;\\n\\n      announceTurn();\\n      if(currentPlayer === CONFIG.PLAYERS.CPU) {\\n        setTimeout(cpuMove, 300);\\n      }\\n    } catch(err) {\\n      console.error(\\'Error starting round:\\', err);\\n    }\\n  }\\n\\n  // ================\\n  // User / Keyboard\\n  // ================\\n  function onCellAction(e){\\n    const isClick = e.type === \\'click\\';\\n    const isEnter = (e.type === \\'keydown\\' && (e.key === \\'Enter\\' || e.key === \\' \\'));\\n    if(!isClick && !isEnter) return;\\n\\n    const idx = Number(e.currentTarget.dataset.idx);\\n    if(gameState !== GameState.IN_PROGRESS ||\\n       board[idx] ||\\n       currentPlayer !== CONFIG.PLAYERS.HUMAN) {\\n      return;\\n    }\\n    makeMove(idx, CONFIG.PLAYERS.HUMAN);\\n    e.preventDefault();\\n  }\\n\\n  // ================\\n  // Core Game Logic\\n  // ================\\n  function makeMove(idx, player){\\n    board[idx] = player;\\n    const cell = cells[idx];\\n    cell.textContent = player;\\n    cell.classList.add(\\'disabled\\');\\n\\n    const winner = checkWinner(board);\\n    if(winner || board.every(v => v)){\\n      endRound(winner || \\'draw\\');\\n    } else {\\n      // Switch turn\\n      currentPlayer = (player === CONFIG.PLAYERS.HUMAN)\\n                      ? CONFIG.PLAYERS.CPU\\n                      : CONFIG.PLAYERS.HUMAN;\\n      announceTurn();\\n      if(currentPlayer === CONFIG.PLAYERS.CPU){\\n        setTimeout(cpuMove, 300);\\n      }\\n    }\\n  }\\n\\n  // End of round: update scores & handle match flow\\n  function endRound(result){\\n    gameState = GameState.ROUND_COMPLETE;\\n    if(result === CONFIG.PLAYERS.HUMAN){\\n      humanScore++;\\n      setMessage(\\'You win this round!\\');\\n    } else if(result === CONFIG.PLAYERS.CPU){\\n      cpuScore++;\\n      setMessage(\\'Computer wins this round!\\');\\n    } else {\\n      setMessage(\"It\\'s a draw.\");\\n    }\\n    updateScores();\\n\\n    // Is match over?\\n    if(humanScore === CONFIG.WINNING_SCORE ||\\n       cpuScore   === CONFIG.WINNING_SCORE ||\\n       roundNum   === CONFIG.MATCH_ROUNDS) {\\n\\n      gameState = GameState.MATCH_COMPLETE;\\n      const champion = humanScore > cpuScore\\n                       ? \\'You win the match!\\'\\n                       : humanScore < cpuScore\\n                         ? \\'Computer wins the match!\\'\\n                         : \"Match is a tie!\";\\n      setTimeout(()=> setMessage(champion), 500);\\n      nextBtn.disabled = true;\\n    } else {\\n      // Prep for next round\\n      roundNum++;\\n      nextBtn.disabled = false;\\n    }\\n  }\\n\\n  function announceTurn(){\\n    if(currentPlayer === CONFIG.PLAYERS.HUMAN){\\n      setMessage(\"Your turn (X).\");\\n    } else {\\n      setMessage(\"Computer\\'s turn (O).\");\\n    }\\n  }\\n\\n  function updateScores(){\\n    humanScoreEl.textContent = humanScore;\\n    cpuScoreEl.textContent   = cpuScore;\\n  }\\n\\n  // ================\\n  // Computer AI\\n  // ================\\n  function cpuMove(){\\n    if(gameState !== GameState.IN_PROGRESS) return;\\n    const { idx } = minimax(board.slice(), CONFIG.PLAYERS.CPU, 0, -Infinity, Infinity);\\n    makeMove(idx, CONFIG.PLAYERS.CPU);\\n  }\\n\\n  /**\\n   * Minimax with alpha-beta pruning and depth‐based scoring.\\n   * + Higher score for quicker CPU wins, lower for longer losses.\\n   */\\n  function minimax(pos, player, depth, alpha, beta) {\\n    const HUMAN = CONFIG.PLAYERS.HUMAN, CPU = CONFIG.PLAYERS.CPU;\\n    const winner = checkWinner(pos);\\n\\n    // Terminal states\\n    if(winner === CPU)   return { score: 10 - depth };\\n    if(winner === HUMAN) return { score: depth - 10 };\\n    if(pos.every(v => v)) return { score: 0 };  // draw\\n\\n    let bestMove = { idx: -1, score: (player === CPU) ? -Infinity : Infinity };\\n\\n    for(let i=0; i<pos.length; i++){\\n      if(pos[i]) continue;\\n      pos[i] = player;\\n      const next = minimax(pos, player===CPU ? HUMAN : CPU, depth+1, alpha, beta);\\n      pos[i] = null;\\n      next.idx = i;\\n\\n      if(player === CPU){\\n        if(next.score > bestMove.score) bestMove = next;\\n        alpha = Math.max(alpha, next.score);\\n      } else {\\n        if(next.score < bestMove.score) bestMove = next;\\n        beta = Math.min(beta, next.score);\\n      }\\n      if(beta <= alpha) break;  // prune\\n    }\\n    return bestMove;\\n  }\\n\\n  // Check all win lines\\n  function checkWinner(bd){\\n    const L = CONFIG.BOARD_SIZE;\\n    const lines = [\\n      [0,1,2],[3,4,5],[6,7,8],  // rows\\n      [0,3,6],[1,4,7],[2,5,8],  // cols\\n      [0,4,8],[2,4,6]           // diags\\n    ];\\n    for(const [a,b,c] of lines){\\n      if(bd[a] && bd[a] === bd[b] && bd[a] === bd[c]){\\n        return bd[a];\\n      }\\n    }\\n    return null;\\n  }\\n\\n  // Utility to show messages\\n  function setMessage(txt){\\n    msgEl.textContent = txt;\\n  }\\n\\n  // Bootstrap\\n  init();\\n\\n})();\\n</script>\\n\\n</body>\\n</html>\\n```\\n\\nWhat’s been improved?\\n\\n1. **CONFIG & GameState**  \\n   – Easy adjustments (board size, rounds, symbols, win-score).  \\n2. **Comments & Strict Mode**  \\n   – Clarify complex logic, especially minimax.  \\n3. **Minimax**  \\n   – Depth scoring (fast wins) + alpha-beta pruning.  \\n4. **Accessibility**  \\n   – `role=\"button\"`, `tabindex`, `aria-label` on cells, and `aria-live` messaging.  \\n5. **Error Handling**  \\n   – Try/catch around round start.  \\n\\nFeel free to extract CSS/JS into separate files, add ARIA improvements or tests as needed!'},\n",
       " {'role': 'user',\n",
       "  'content': 'Here is the review feedback:\\nCode Review Feedback:\\n\\nStrengths:\\n1. **Excellent Modular Design**\\n   - Well-structured single-page application\\n   - Encapsulated in an IIFE to prevent global scope pollution\\n   - Clear separation of concerns (initialization, game logic, UI)\\n\\n2. **Configuration & Extensibility**\\n   - `CONFIG` object allows easy customization of game parameters\\n   - `GameState` enum provides clear state management\\n   - Flexible board size and match configuration\\n\\n3. **AI Implementation**\\n   - Sophisticated minimax algorithm with alpha-beta pruning\\n   - Depth-based scoring for more intelligent move selection\\n   - Handles edge cases like draws effectively\\n\\n4. **Accessibility Improvements**\\n   - ARIA roles and labels\\n   - Keyboard support\\n   - Live region for game status announcements\\n\\n5. **Error Handling**\\n   - Basic error catching in critical sections\\n   - Defensive programming techniques\\n\\nSuggestions for Improvement:\\n\\n1. **Performance Optimization**\\n   - Consider memoization for minimax to cache repeated board states\\n   - Implement a depth limit to prevent excessive computation\\n\\n2. **Code Quality**\\n   ```javascript\\n   // Consider extracting complex methods\\n   function minimax(pos, player, depth, alpha, beta) {\\n     // This could benefit from being split into smaller, more focused functions\\n     // e.g., separate terminal state checking, move generation\\n   }\\n   ```\\n\\n3. **Accessibility Enhancements**\\n   - Add more detailed error messages\\n   - Improve color contrast for better readability\\n   - Consider adding more detailed screen reader instructions\\n\\n4. **Testing**\\n   - Add unit tests for core game logic\\n   - Implement integration tests for game flow\\n\\n5. **Code Documentation**\\n   ```javascript\\n   /**\\n    * Performs minimax algorithm with alpha-beta pruning\\n    * @param {Array} pos - Current board state\\n    * @param {string} player - Current player\\n    * @param {number} depth - Current search depth\\n    * @param {number} alpha - Alpha pruning value\\n    * @param {number} beta - Beta pruning value\\n    * @returns {Object} Best move and its score\\n    */\\n   function minimax(pos, player, depth, alpha, beta) {\\n     // Consider adding more inline comments explaining complex logic\\n   }\\n   ```\\n\\nPotential Architectural Improvements:\\n1. Consider separating concerns further:\\n   - Move game logic into a separate class/module\\n   - Create a distinct UI controller\\n   - Implement a more robust state management pattern\\n\\nCode Quality Rating: 8/10\\n\\nRecommendations:\\n1. Add comprehensive error logging\\n2. Implement more robust input validation\\n3. Create a configuration validation method\\n4. Consider adding difficulty levels for AI\\n\\nOverall, this is a well-implemented Tic-Tac-Toe application with strong design principles and thoughtful implementation. The code demonstrates a good understanding of game development, AI algorithms, and frontend best practices.\\n\\nWould you like me to elaborate on any specific aspect of the code or provide a more detailed analysis of any particular section?\\n                              Please revise the code accordingly.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Below is an updated version of the single-page Tic-Tac-Toe app incorporating your latest feedback:\\n\\nKey Enhancements  \\n• Memoization & depth-limit for minimax  \\n• Extracted terminal-state check & move-generation helpers  \\n• Full JSDoc on minimax and helpers  \\n• CONFIG validation on init  \\n• Global error logging  \\n• Improved color contrast & screen-reader instructions  \\n• Basic difficulty selector stub (Easy/Hard)  \\n\\nSave as `index.html` and open in your browser.\\n\\n```html\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n  <meta charset=\"UTF-8\">\\n  <title>Tic-Tac-Toe: Human vs Computer</title>\\n  <style>\\n    /* === Reset & Base === */\\n    * { box-sizing: border-box; margin:0; padding:0 }\\n    body {\\n      font-family: sans-serif;\\n      background: #ececec;\\n      color: #111;\\n      text-align: center;\\n      padding: 20px;\\n    }\\n    /* === Accessibility Note === */\\n    #instructions {\\n      margin-bottom: 10px;\\n      font-size: 0.9em;\\n      color: #333;\\n    }\\n    /* === Scoreboard === */\\n    #scoreboard { margin-bottom: 10px; }\\n    #message {\\n      min-height: 1.5em;\\n      margin-bottom: 10px;\\n      font-weight: bold;\\n      color: #00529B;\\n    }\\n    /* === Board Grid === */\\n    #board {\\n      display: grid;\\n      grid-template: repeat(3, 80px) / repeat(3, 80px);\\n      gap: 5px;\\n      margin: 0 auto 10px;\\n      width: max-content;\\n    }\\n    .cell {\\n      background: #FFF;\\n      border: 2px solid #333;\\n      font-size: 2em;\\n      display: flex;\\n      align-items: center;\\n      justify-content: center;\\n      cursor: pointer;\\n      user-select: none;\\n    }\\n    .cell:focus {\\n      outline: 3px solid #3399FF;\\n    }\\n    .cell.disabled {\\n      cursor: default;\\n      color: #666;\\n    }\\n    /* === Buttons & Select === */\\n    button, select {\\n      padding: 8px 12px;\\n      margin: 0 5px;\\n      font-size: 1em;\\n      cursor: pointer;\\n    }\\n    button:disabled, select:disabled {\\n      opacity: 0.5;\\n      cursor: default;\\n    }\\n  </style>\\n</head>\\n<body>\\n\\n  <div id=\"instructions\" role=\"note\">\\n    Use mouse or keyboard (Enter/Space) to play. First to 3 wins in 5 rounds.\\n  </div>\\n\\n  <div id=\"scoreboard\">\\n    <div>Round <span id=\"round-num\">1</span> of <span id=\"total-rounds\"></span></div>\\n    <div>\\n      <span>You (X): <span id=\"human-score\">0</span></span>\\n       – \\n      <span>Computer (O): <span id=\"computer-score\">0</span></span>\\n    </div>\\n  </div>\\n\\n  <label for=\"difficulty\">AI Difficulty:</label>\\n  <select id=\"difficulty\">\\n    <option value=\"hard\" selected>Hard (Minimax)</option>\\n    <option value=\"easy\">Easy (Random)</option>\\n  </select>\\n\\n  <div id=\"message\" aria-live=\"polite\"></div>\\n\\n  <div id=\"board\" role=\"grid\" aria-label=\"Tic-Tac-Toe board\"></div>\\n\\n  <div>\\n    <button id=\"next-round\" disabled>Next Round</button>\\n    <button id=\"reset-match\">Reset Match</button>\\n  </div>\\n\\n<script>\\n(function(){\\n  \\'use strict\\';\\n\\n  // ======= CONFIG & ENUMS =======\\n  const CONFIG = {\\n    BOARD_SIZE:   3,\\n    MATCH_ROUNDS: 5,\\n    WIN_SCORE:    3,\\n    PLAYERS:      { HUMAN: \\'X\\', CPU: \\'O\\' }\\n  };\\n  const GameState = {\\n    INIT:    \\'initialized\\',\\n    PLAY:    \\'in_progress\\',\\n    ROUND:   \\'round_complete\\',\\n    MATCH:   \\'match_complete\\'\\n  };\\n\\n  // Validate CONFIG\\n  if (CONFIG.BOARD_SIZE**2 !== 9) {\\n    console.error(\\'CONFIG.BOARD_SIZE currently must be 3\\');\\n  }\\n\\n  // ====== STATE ======\\n  let board, humanScore, cpuScore, roundNum, currentPlayer, gameState;\\n  let memo = new Map();            // minimax cache\\n  const cells = [];\\n\\n  // ====== DOM ======\\n  const boardEl       = document.getElementById(\\'board\\');\\n  const msgEl         = document.getElementById(\\'message\\');\\n  const humanScoreEl  = document.getElementById(\\'human-score\\');\\n  const cpuScoreEl    = document.getElementById(\\'computer-score\\');\\n  const roundNumEl    = document.getElementById(\\'round-num\\');\\n  const totalRoundsEl = document.getElementById(\\'total-rounds\\');\\n  const nextBtn       = document.getElementById(\\'next-round\\');\\n  const resetBtn      = document.getElementById(\\'reset-match\\');\\n  const diffEl        = document.getElementById(\\'difficulty\\');\\n\\n  // ===== GLOBAL ERROR LOGGER =====\\n  window.addEventListener(\\'error\\', e => {\\n    console.error(\\'Unhandled error:\\', e.message, \\'at\\', e.filename + \\':\\' + e.lineno);\\n  });\\n\\n  // ===== INIT =====\\n  function init(){\\n    totalRoundsEl.textContent = CONFIG.MATCH_ROUNDS;\\n    buildBoard();\\n    bindControls();\\n    resetMatch();\\n  }\\n\\n  // Build cells with ARIA & keyboard support\\n  function buildBoard(){\\n    boardEl.innerHTML = \\'\\';\\n    for(let i=0; i<CONFIG.BOARD_SIZE**2; i++){\\n      const div = document.createElement(\\'div\\');\\n      div.className        = \\'cell\\';\\n      div.setAttribute(\\'role\\',\\'button\\');\\n      div.setAttribute(\\'tabindex\\',\\'0\\');\\n      div.setAttribute(\\'aria-label\\', `Cell ${Math.floor(i/3)+1}, ${i%3+1}`);\\n      div.dataset.idx      = i;\\n      div.addEventListener(\\'click\\', onCellTrigger);\\n      div.addEventListener(\\'keydown\\', onCellTrigger);\\n      boardEl.appendChild(div);\\n      cells[i] = div;\\n    }\\n  }\\n\\n  function bindControls(){\\n    nextBtn.addEventListener(\\'click\\', startRound);\\n    resetBtn.addEventListener(\\'click\\', resetMatch);\\n  }\\n\\n  // ===== MATCH LIFECYCLE =====\\n  function resetMatch(){\\n    humanScore = cpuScore = 0;\\n    roundNum   = 1;\\n    gameState  = GameState.INIT;\\n    updateScores();\\n    roundNumEl.textContent = roundNum;\\n    nextBtn.disabled = true;\\n    setMessage(\\'Press \"Next Round\" to begin\\');\\n  }\\n\\n  function startRound(){\\n    board = Array(CONFIG.BOARD_SIZE**2).fill(null);\\n    gameState = GameState.PLAY;\\n    memo.clear();  // reset cache each round\\n    cells.forEach(c => {\\n      c.textContent = \\'\\';\\n      c.classList.remove(\\'disabled\\');\\n    });\\n    roundNumEl.textContent = roundNum;\\n    nextBtn.disabled = true;\\n\\n    // Alternate starter\\n    currentPlayer = (roundNum % 2 === 1)\\n                    ? CONFIG.PLAYERS.HUMAN\\n                    : CONFIG.PLAYERS.CPU;\\n    announceTurn();\\n    if(currentPlayer === CONFIG.PLAYERS.CPU){\\n      setTimeout(cpuMove, 200);\\n    }\\n  }\\n\\n  // ===== USER ACTION =====\\n  function onCellTrigger(e){\\n    const click  = e.type === \\'click\\';\\n    const enter  = e.type===\\'keydown\\' && (e.key===\\'Enter\\'||e.key===\\' \\');\\n    if(!click && !enter) return;\\n    const idx = +e.currentTarget.dataset.idx;\\n    if(gameState !== GameState.PLAY ||\\n       board[idx] ||\\n       currentPlayer !== CONFIG.PLAYERS.HUMAN) return;\\n    makeMove(idx, CONFIG.PLAYERS.HUMAN);\\n    e.preventDefault();\\n  }\\n\\n  // ===== CORE LOGIC =====\\n  function makeMove(idx, player){\\n    try {\\n      board[idx] = player;\\n      const cell = cells[idx];\\n      cell.textContent = player;\\n      cell.classList.add(\\'disabled\\');\\n\\n      const winner = checkWinner(board);\\n      if(winner || board.every(v=>v)){\\n        endRound(winner || \\'draw\\');\\n      } else {\\n        currentPlayer = (player === CONFIG.PLAYERS.HUMAN)\\n                        ? CONFIG.PLAYERS.CPU\\n                        : CONFIG.PLAYERS.HUMAN;\\n        announceTurn();\\n        if(currentPlayer===CONFIG.PLAYERS.CPU){\\n          setTimeout(cpuMove, 300);\\n        }\\n      }\\n    } catch(err) {\\n      console.error(\\'makeMove error:\\', err);\\n    }\\n  }\\n\\n  function endRound(result){\\n    gameState = GameState.ROUND;\\n    if(result === CONFIG.PLAYERS.HUMAN){\\n      humanScore++;\\n      setMessage(\\'You win this round!\\');\\n    } else if(result === CONFIG.PLAYERS.CPU){\\n      cpuScore++;\\n      setMessage(\\'Computer wins this round!\\');\\n    } else {\\n      setMessage(\"It\\'s a draw.\");\\n    }\\n    updateScores();\\n\\n    // Match over?\\n    if(humanScore===CONFIG.WIN_SCORE ||\\n       cpuScore===CONFIG.WIN_SCORE ||\\n       roundNum===CONFIG.MATCH_ROUNDS) {\\n\\n      gameState = GameState.MATCH;\\n      const finalMsg = humanScore>cpuScore\\n                       ? \\'You win the match!\\'\\n                       : humanScore<cpuScore\\n                         ? \\'Computer wins the match!\\'\\n                         : \"Match is a tie!\";\\n      setTimeout(()=> setMessage(finalMsg), 500);\\n      nextBtn.disabled = true;\\n    } else {\\n      roundNum++;\\n      nextBtn.disabled = false;\\n    }\\n  }\\n\\n  function announceTurn(){\\n    if(currentPlayer===CONFIG.PLAYERS.HUMAN){\\n      setMessage(\"Your turn (X).\");\\n    } else {\\n      setMessage(\"Computer\\'s turn (O).\");\\n    }\\n  }\\n\\n  function updateScores(){\\n    humanScoreEl.textContent = humanScore;\\n    cpuScoreEl.textContent   = cpuScore;\\n  }\\n\\n  // ===== CPU MOVE =====\\n  function cpuMove(){\\n    if(gameState !== GameState.PLAY) return;\\n    const difficulty = diffEl.value;\\n    let idx;\\n    if(difficulty === \\'easy\\') {\\n      // Random available\\n      const avail = board.map((v,i)=>v?null:i).filter(i=>i!==null);\\n      idx = avail[Math.floor(Math.random()*avail.length)];\\n    } else {\\n      // Hard: minimax\\n      const result = minimax(board.slice(), CONFIG.PLAYERS.CPU, 0, -Infinity, Infinity);\\n      idx = result.idx;\\n    }\\n    makeMove(idx, CONFIG.PLAYERS.CPU);\\n  }\\n\\n  // ===== MINIMAX HELPERS =====\\n\\n  /**\\n   * Check for terminal state quickly.\\n   * @param {Array} bd - board array\\n   * @return {string|null} \\'X\\' or \\'O\\', or null if no winner\\n   */\\n  function checkWinner(bd){\\n    const lines = [\\n      [0,1,2],[3,4,5],[6,7,8],\\n      [0,3,6],[1,4,7],[2,5,8],\\n      [0,4,8],[2,4,6]\\n    ];\\n    for(const [a,b,c] of lines){\\n      if(bd[a] && bd[a]===bd[b] && bd[a]===bd[c]){\\n        return bd[a];\\n      }\\n    }\\n    return null;\\n  }\\n\\n  /**\\n   * Generate list of available move indices.\\n   * @param {Array} bd - board array\\n   * @return {number[]} free cell indices\\n   */\\n  function availableMoves(bd){\\n    return bd.map((v,i)=>v?null:i).filter(i=>i!==null);\\n  }\\n\\n  /**\\n   * Minimax with alpha-beta pruning, depth limit, and memoization.\\n   * @param {Array} pos     - current board\\n   * @param {string} player - current mover (\\'X\\' or \\'O\\')\\n   * @param {number} depth  - recursion depth\\n   * @param {number} alpha  - alpha value\\n   * @param {number} beta   - beta value\\n   * @returns {{idx:number,score:number}} best move\\n   */\\n  function minimax(pos, player, depth, alpha, beta){\\n    const key = pos.join(\\'\\') + player;\\n    if(memo.has(key)) return memo.get(key);\\n\\n    const winner = checkWinner(pos);\\n    if(winner === CONFIG.PLAYERS.CPU) {\\n      return { idx:-1, score: 10 - depth };\\n    }\\n    if(winner === CONFIG.PLAYERS.HUMAN) {\\n      return { idx:-1, score: depth - 10 };\\n    }\\n    const avail = availableMoves(pos);\\n    if(avail.length === 0 || depth >= 9) {\\n      return { idx:-1, score: 0 }; // draw or depth limit\\n    }\\n\\n    let best = {\\n      idx: (player===CONFIG.PLAYERS.CPU ? -1 : -1),\\n      score: (player===CONFIG.PLAYERS.CPU ? -Infinity : Infinity)\\n    };\\n\\n    for(const i of avail){\\n      pos[i] = player;\\n      const next = minimax(pos,\\n                           player===CONFIG.PLAYERS.CPU\\n                             ? CONFIG.PLAYERS.HUMAN\\n                             : CONFIG.PLAYERS.CPU,\\n                           depth+1, alpha, beta);\\n      pos[i] = null;\\n      next.idx = i;\\n\\n      if(player===CONFIG.PLAYERS.CPU){\\n        if(next.score > best.score) best = next;\\n        alpha = Math.max(alpha, best.score);\\n      } else {\\n        if(next.score < best.score) best = next;\\n        beta = Math.min(beta, best.score);\\n      }\\n      if(beta <= alpha) break;\\n    }\\n\\n    memo.set(key, best);\\n    return best;\\n  }\\n\\n  function setMessage(txt){\\n    msgEl.textContent = txt;\\n  }\\n\\n  // ===== BOOTSTRAP =====\\n  init();\\n\\n})();\\n</script>\\n\\n</body>\\n</html>\\n```\\n\\nSummary of changes  \\n1. Added a `difficulty` selector (Easy: random moves; Hard: minimax).  \\n2. Minimax now uses:  \\n   • Memoization (`Map`)  \\n   • Depth limit (stops at depth 9)  \\n   • Separated `checkWinner` & `availableMoves` helpers  \\n   • Full JSDoc comments  \\n3. Global `window.onerror` logger for uncaught errors.  \\n4. Improved color contrast and added an ARIA “note” with basic play instructions.  \\n5. CONFIG validation at startup.  \\n\\nFeel free to extract modules, add unit tests, or expand accessibility instructions further!'},\n",
       " {'role': 'user',\n",
       "  'content': \"Here is the review feedback:\\nCode Review Feedback:\\n\\nOverall Strengths:\\n✅ Excellent implementation of a single-page Tic-Tac-Toe game\\n✅ Strong focus on code organization and modularity\\n✅ Comprehensive accessibility considerations\\n✅ Robust error handling\\n✅ Advanced AI implementation with minimax algorithm\\n\\nSpecific Positive Observations:\\n1. Minimax Algorithm\\n   - Excellent implementation of alpha-beta pruning\\n   - Memoization for performance optimization\\n   - Depth-limited search to prevent excessive recursion\\n   - Clear, well-documented helper functions\\n\\n2. Accessibility\\n   - ARIA attributes for screen readers\\n   - Keyboard navigation support\\n   - Clear instructions\\n   - Semantic HTML structure\\n\\n3. State Management\\n   - Comprehensive game state tracking\\n   - Clear separation of concerns\\n   - Robust error handling with global error logger\\n\\nSuggestions for Improvement:\\n\\n1. Configuration Management\\n```javascript\\n// Consider using Object.freeze for immutability\\nconst CONFIG = Object.freeze({\\n  BOARD_SIZE: 3,\\n  MATCH_ROUNDS: 5,\\n  WIN_SCORE: 3,\\n  PLAYERS: Object.freeze({ HUMAN: 'X', CPU: 'O' })\\n});\\n```\\n\\n2. Dependency Injection for Difficulty Strategy\\n```javascript\\n// Create a strategy pattern for move selection\\nconst difficultyStrategies = {\\n  easy: (board) => {\\n    const available = board.map((v,i) => v ? null : i).filter(i => i !== null);\\n    return available[Math.floor(Math.random() * available.length)];\\n  },\\n  hard: (board) => {\\n    const result = minimax(board.slice(), CONFIG.PLAYERS.CPU, 0, -Infinity, Infinity);\\n    return result.idx;\\n  }\\n};\\n\\nfunction cpuMove() {\\n  const strategy = difficultyStrategies[diffEl.value];\\n  const idx = strategy(board);\\n  makeMove(idx, CONFIG.PLAYERS.CPU);\\n}\\n```\\n\\n3. Performance Optimization for Minimax\\n```javascript\\n// Consider limiting memoization cache size\\nfunction limitMemoCache(memo, maxSize = 1000) {\\n  if (memo.size > maxSize) {\\n    const oldestKey = memo.keys().next().value;\\n    memo.delete(oldestKey);\\n  }\\n}\\n```\\n\\n4. Enhanced Error Handling\\n```javascript\\n// More granular error handling\\nwindow.addEventListener('error', (e) => {\\n  const errorDetails = {\\n    message: e.message,\\n    filename: e.filename,\\n    lineno: e.lineno,\\n    timestamp: new Date().toISOString()\\n  };\\n  console.error('Unhandled Error:', JSON.stringify(errorDetails));\\n  // Optional: Send to error tracking service\\n});\\n```\\n\\n5. Potential Enhancements\\n- Add unit tests for core game logic\\n- Implement game replay/undo functionality\\n- Add sound effects and animations\\n- Create difficulty levels with varying minimax depth\\n\\nRecommendations for Future Development:\\n1. Extract JavaScript into separate module\\n2. Implement comprehensive test suite\\n3. Add persistent scoring/statistics\\n4. Create responsive design for mobile\\n\\nRating: 4.5/5 🌟\\nAn impressively well-structured, accessible, and performant implementation of Tic-Tac-Toe with advanced AI capabilities.\\n\\nWould you like me to elaborate on any specific aspect of the code or review?\\n                              Please revise the code accordingly.\"}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_model.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b447b771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
